{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Meta, TFIDF, and DocEmbedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from mlxtend import classifier\n",
    "from mlxtend import feature_selection\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "import sklearn\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# import en_core_web_sm\n",
    "# from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = stop_words.union({'th','st'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "punctuation = punctuation+\"...\"+\"--\"+\"“\"+\"”\"+\"``\"+\"''\"+\"’\"+\"–\"+\"—\"+\"‘\"\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['I’m', 'won’t', '’s', '’ll', '’ve ', 'n’t', '’re', '’d', 'y’all', \"I'm\", \"won't\", \"'s\", \"'ll\", \"'ve \", \"n't\", \"'re\", \"'d\", \"y'all\"])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contr_dict={\"I’m\": \"I am\",\n",
    "            \"won’t\": \"will not\",\n",
    "            \"’s\" : \"\", \n",
    "            \"’ll\":\"will\",\n",
    "            \"’ve \":\"have \",\n",
    "            \"n’t\":\" not\",\n",
    "            \"’re\": \"are\",\n",
    "            \"’d\": \"would\",\n",
    "            \"y’all\": \"all of you\",\n",
    "            \"I'm\": \"I am\",\n",
    "            \"won't\": \"will not\",\n",
    "            \"'s\" : \"\", \n",
    "            \"'ll\":\"will\",\n",
    "            \"'ve \":\"have \",\n",
    "            \"n't\":\"not\",\n",
    "            \"'re\": \"are\",\n",
    "            \"'d\": \"would\",\n",
    "            \"y'all\": \"all of you\"}\n",
    "contr_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(sentence, contr_dict=contr_dict):\n",
    "    for contr in contr_dict.keys():\n",
    "        if contr in sentence:\n",
    "            sentence = sentence.replace(contr,contr_dict[contr])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "  \n",
    "def remove_numbers(tokens): \n",
    "    pattern = '[0-9]'\n",
    "    tokens_updated = [re.sub(pattern, '', token) for token in tokens] \n",
    "    return tokens_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert nltk tag to wordnet tag\n",
    "# this is important because having the POS tag improves lemmatization\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization and lemmatization function\n",
    "def tokenize_sentence(sentence):\n",
    "    #remove contractions\n",
    "    sentence = replace_contractions(sentence, contr_dict=contr_dict)\n",
    "    \n",
    "    #tokenize the sentence\n",
    "    mytokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    #remove numbers\n",
    "    mytokens = remove_numbers(mytokens)\n",
    "    \n",
    "    #remove tokens left over that are only space char\n",
    "    mytokens = [token for token in mytokens if len(token)>0]\n",
    "    \n",
    "    #tag tokens with part of speech\n",
    "    nltk_tagged = nltk.pos_tag(mytokens)\n",
    "\n",
    "    # remove punctuation\n",
    "    nltk_tagged = [ word for word in nltk_tagged if word[0] not in punctuation ]\n",
    "    \n",
    "    #\n",
    "    nltk_tagged = [word for word in nltk_tagged if word[0].isalpha()]\n",
    "    \n",
    "    # strip all tokens and make lowercase \n",
    "    nltk_tagged = [ (word[0].lower().strip(),word[1]) for word in nltk_tagged ]\n",
    "    \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_tokens.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(word, tag))\n",
    "            \n",
    "        lemmatized_tokens = [word for word in lemmatized_tokens if word not in STOP_WORDS]\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA FRAME SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('meta_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>propaganda</th>\n",
       "      <th>propaganda_type</th>\n",
       "      <th>text</th>\n",
       "      <th>prop_txt_snippet</th>\n",
       "      <th>sent_#</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>strong_subjectives_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Florida Muslim Leader Sofian Zakkout’s D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.444444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>701225819</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Name_Calling,Labeling</td>\n",
       "      <td>David Duke, the white supremacist icon and for...</td>\n",
       "      <td>Grand Wizard of the Ku Klux Klan</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>4.423077</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>701225819</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>However, one individual who represents the Mus...</td>\n",
       "      <td>enamored</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Last month, once again, Zakkout chose to showc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The postings can be rivaled only by Zakkout’s ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id      propaganda        propaganda_type  \\\n",
       "0   701225819  non-propaganda                    NaN   \n",
       "1   701225819      propaganda  Name_Calling,Labeling   \n",
       "2   701225819      propaganda        Loaded_Language   \n",
       "3   701225819  non-propaganda                    NaN   \n",
       "4   701225819  non-propaganda                    NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  South Florida Muslim Leader Sofian Zakkout’s D...   \n",
       "1  David Duke, the white supremacist icon and for...   \n",
       "2  However, one individual who represents the Mus...   \n",
       "3  Last month, once again, Zakkout chose to showc...   \n",
       "4  The postings can be rivaled only by Zakkout’s ...   \n",
       "\n",
       "                   prop_txt_snippet  sent_#  sentiment_score  abs_sent_score  \\\n",
       "0                               NaN       1           0.0000          0.0000   \n",
       "1  Grand Wizard of the Ku Klux Klan       2           0.5423          0.5423   \n",
       "2                          enamored       3           0.3612          0.3612   \n",
       "3                               NaN       4           0.0000          0.0000   \n",
       "4                               NaN       5           0.0000          0.0000   \n",
       "\n",
       "   punct_count  word_count      %adj     %verb      %adv     %noun  \\\n",
       "0            0           9  0.000000  0.000000  0.000000  0.000000   \n",
       "1            4          26  0.020548  0.006849  0.013699  0.006849   \n",
       "2            4          27  0.017241  0.017241  0.005747  0.022989   \n",
       "3            5          22  0.021127  0.021127  0.014085  0.035211   \n",
       "4            1          11  0.014493  0.043478  0.014493  0.028986   \n",
       "\n",
       "   avg_word_length  strong_subjectives_count  \n",
       "0         5.444444                         0  \n",
       "1         4.423077                         2  \n",
       "2         5.000000                         0  \n",
       "3         5.045455                         0  \n",
       "4         4.636364                         0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15172 entries, 0 to 15171\n",
      "Data columns (total 12 columns):\n",
      "propaganda                  15172 non-null object\n",
      "text                        15172 non-null object\n",
      "sentiment_score             15172 non-null float64\n",
      "abs_sent_score              15172 non-null float64\n",
      "punct_count                 15172 non-null int64\n",
      "word_count                  15172 non-null int64\n",
      "%adj                        15172 non-null float64\n",
      "%verb                       15172 non-null float64\n",
      "%adv                        15172 non-null float64\n",
      "%noun                       15172 non-null float64\n",
      "avg_word_length             15172 non-null float64\n",
      "strong_subjectives_count    15172 non-null int64\n",
      "dtypes: float64(7), int64(3), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "stack_model_df = df.drop(['article_id','propaganda_type','prop_txt_snippet','sent_#'], axis=1)\n",
    "stack_model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stack_model_df['propaganda']\n",
    "X = stack_model_df.drop('propaganda', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = X['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['text_2']=text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'sentiment_score', 'abs_sent_score', 'punct_count',\n",
       "       'word_count', '%adj', '%verb', '%adv', '%noun', 'avg_word_length',\n",
       "       'strong_subjectives_count', 'embeds', 'text_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1 if label == 'propaganda' else 0 for label in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_embeds = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['embeds'] = model_embeds.encode(np.array(X['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_array = np.array(X['embeds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(X['embeds'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15172 entries, 0 to 15171\n",
      "Columns: 768 entries, 0 to 767\n",
      "dtypes: float64(768)\n",
      "memory usage: 88.9 MB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeds = X.join(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeds = X_embeds.drop(['embeds','text_2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15172 entries, 0 to 15171\n",
      "Columns: 779 entries, text to 767\n",
      "dtypes: float64(775), int64(3), object(1)\n",
      "memory usage: 90.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X_embeds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.164120</td>\n",
       "      <td>-0.019730</td>\n",
       "      <td>0.203762</td>\n",
       "      <td>-0.149297</td>\n",
       "      <td>0.696591</td>\n",
       "      <td>-0.960957</td>\n",
       "      <td>0.206785</td>\n",
       "      <td>-0.540251</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>0.196422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075324</td>\n",
       "      <td>-0.620497</td>\n",
       "      <td>-0.303472</td>\n",
       "      <td>0.297969</td>\n",
       "      <td>1.019830</td>\n",
       "      <td>-0.523595</td>\n",
       "      <td>-0.525541</td>\n",
       "      <td>0.337973</td>\n",
       "      <td>0.422184</td>\n",
       "      <td>-0.103847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.107122</td>\n",
       "      <td>0.944672</td>\n",
       "      <td>-0.452394</td>\n",
       "      <td>-0.229796</td>\n",
       "      <td>-0.732791</td>\n",
       "      <td>-0.794121</td>\n",
       "      <td>1.180954</td>\n",
       "      <td>-0.231252</td>\n",
       "      <td>0.270980</td>\n",
       "      <td>0.060620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069693</td>\n",
       "      <td>0.726365</td>\n",
       "      <td>-0.519457</td>\n",
       "      <td>-1.590454</td>\n",
       "      <td>0.438942</td>\n",
       "      <td>-0.663646</td>\n",
       "      <td>0.016005</td>\n",
       "      <td>-0.209250</td>\n",
       "      <td>-0.010170</td>\n",
       "      <td>0.619601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.123370</td>\n",
       "      <td>0.182647</td>\n",
       "      <td>0.767236</td>\n",
       "      <td>-0.386340</td>\n",
       "      <td>0.239815</td>\n",
       "      <td>-1.276435</td>\n",
       "      <td>0.194789</td>\n",
       "      <td>-0.089553</td>\n",
       "      <td>0.052065</td>\n",
       "      <td>-0.120395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.506447</td>\n",
       "      <td>-1.584028</td>\n",
       "      <td>-0.943574</td>\n",
       "      <td>-1.807983</td>\n",
       "      <td>0.863276</td>\n",
       "      <td>-0.925124</td>\n",
       "      <td>-0.041773</td>\n",
       "      <td>0.223409</td>\n",
       "      <td>-0.194369</td>\n",
       "      <td>-0.310801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.212266</td>\n",
       "      <td>0.199948</td>\n",
       "      <td>1.093049</td>\n",
       "      <td>-0.079362</td>\n",
       "      <td>0.368825</td>\n",
       "      <td>-0.464936</td>\n",
       "      <td>0.845249</td>\n",
       "      <td>0.332470</td>\n",
       "      <td>-0.713169</td>\n",
       "      <td>-0.262536</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256164</td>\n",
       "      <td>-1.225924</td>\n",
       "      <td>-1.114888</td>\n",
       "      <td>-0.392965</td>\n",
       "      <td>0.123237</td>\n",
       "      <td>-0.763239</td>\n",
       "      <td>-0.119133</td>\n",
       "      <td>0.222245</td>\n",
       "      <td>0.562734</td>\n",
       "      <td>0.133977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.082211</td>\n",
       "      <td>0.736390</td>\n",
       "      <td>0.620443</td>\n",
       "      <td>0.304812</td>\n",
       "      <td>0.169626</td>\n",
       "      <td>-0.476543</td>\n",
       "      <td>0.072635</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>-0.055084</td>\n",
       "      <td>0.132189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188720</td>\n",
       "      <td>0.061769</td>\n",
       "      <td>0.530789</td>\n",
       "      <td>-1.138050</td>\n",
       "      <td>-0.434086</td>\n",
       "      <td>0.134580</td>\n",
       "      <td>0.068409</td>\n",
       "      <td>-0.174906</td>\n",
       "      <td>0.013871</td>\n",
       "      <td>0.111412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.504248</td>\n",
       "      <td>0.465273</td>\n",
       "      <td>0.330162</td>\n",
       "      <td>-0.282355</td>\n",
       "      <td>0.195496</td>\n",
       "      <td>-1.244920</td>\n",
       "      <td>0.398042</td>\n",
       "      <td>0.102070</td>\n",
       "      <td>-0.213497</td>\n",
       "      <td>0.194332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177531</td>\n",
       "      <td>-0.336883</td>\n",
       "      <td>-0.527773</td>\n",
       "      <td>0.365958</td>\n",
       "      <td>0.612401</td>\n",
       "      <td>-0.599148</td>\n",
       "      <td>-0.148439</td>\n",
       "      <td>-0.568717</td>\n",
       "      <td>0.717854</td>\n",
       "      <td>0.218804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.228612</td>\n",
       "      <td>0.261782</td>\n",
       "      <td>-0.260801</td>\n",
       "      <td>-0.214646</td>\n",
       "      <td>0.547143</td>\n",
       "      <td>-0.754278</td>\n",
       "      <td>0.029804</td>\n",
       "      <td>-0.378770</td>\n",
       "      <td>-0.459770</td>\n",
       "      <td>-0.506010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112834</td>\n",
       "      <td>-0.710977</td>\n",
       "      <td>-0.672821</td>\n",
       "      <td>0.589150</td>\n",
       "      <td>0.225079</td>\n",
       "      <td>-0.839100</td>\n",
       "      <td>0.156029</td>\n",
       "      <td>0.297599</td>\n",
       "      <td>1.139234</td>\n",
       "      <td>-0.182229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.090555</td>\n",
       "      <td>0.529084</td>\n",
       "      <td>0.356458</td>\n",
       "      <td>-0.081107</td>\n",
       "      <td>0.355607</td>\n",
       "      <td>-0.283721</td>\n",
       "      <td>0.824368</td>\n",
       "      <td>0.386737</td>\n",
       "      <td>-0.593540</td>\n",
       "      <td>-0.308293</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.310062</td>\n",
       "      <td>-0.642304</td>\n",
       "      <td>-1.027118</td>\n",
       "      <td>0.195643</td>\n",
       "      <td>0.058157</td>\n",
       "      <td>-0.368076</td>\n",
       "      <td>-0.401911</td>\n",
       "      <td>0.302515</td>\n",
       "      <td>0.576539</td>\n",
       "      <td>0.144626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010008</td>\n",
       "      <td>0.962858</td>\n",
       "      <td>-0.633553</td>\n",
       "      <td>-0.451542</td>\n",
       "      <td>-0.093861</td>\n",
       "      <td>-1.025709</td>\n",
       "      <td>0.891725</td>\n",
       "      <td>0.094641</td>\n",
       "      <td>0.341399</td>\n",
       "      <td>0.166679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415344</td>\n",
       "      <td>-0.358977</td>\n",
       "      <td>-1.049884</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>0.391501</td>\n",
       "      <td>-0.288238</td>\n",
       "      <td>-0.054236</td>\n",
       "      <td>-0.175665</td>\n",
       "      <td>1.020589</td>\n",
       "      <td>0.036927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.131621</td>\n",
       "      <td>0.445831</td>\n",
       "      <td>-0.713069</td>\n",
       "      <td>-0.268536</td>\n",
       "      <td>-0.454286</td>\n",
       "      <td>-0.289466</td>\n",
       "      <td>0.878650</td>\n",
       "      <td>0.204928</td>\n",
       "      <td>-0.058836</td>\n",
       "      <td>0.309174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.374667</td>\n",
       "      <td>-0.532018</td>\n",
       "      <td>-0.597064</td>\n",
       "      <td>-0.810273</td>\n",
       "      <td>0.433057</td>\n",
       "      <td>-0.209578</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>-0.561577</td>\n",
       "      <td>-0.110241</td>\n",
       "      <td>-0.092543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.291645</td>\n",
       "      <td>0.150018</td>\n",
       "      <td>0.474314</td>\n",
       "      <td>0.366117</td>\n",
       "      <td>0.266019</td>\n",
       "      <td>-0.231297</td>\n",
       "      <td>0.071569</td>\n",
       "      <td>0.231684</td>\n",
       "      <td>-0.013936</td>\n",
       "      <td>0.295219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179121</td>\n",
       "      <td>-0.125835</td>\n",
       "      <td>0.485977</td>\n",
       "      <td>-0.688165</td>\n",
       "      <td>-0.111847</td>\n",
       "      <td>-0.687745</td>\n",
       "      <td>-0.606426</td>\n",
       "      <td>0.098401</td>\n",
       "      <td>-0.434292</td>\n",
       "      <td>0.285049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.598658</td>\n",
       "      <td>0.033788</td>\n",
       "      <td>2.198835</td>\n",
       "      <td>-0.107228</td>\n",
       "      <td>0.247022</td>\n",
       "      <td>-0.053831</td>\n",
       "      <td>-0.721075</td>\n",
       "      <td>0.855839</td>\n",
       "      <td>-0.312170</td>\n",
       "      <td>-0.006771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285903</td>\n",
       "      <td>-0.690131</td>\n",
       "      <td>0.990308</td>\n",
       "      <td>-0.683531</td>\n",
       "      <td>0.219738</td>\n",
       "      <td>-0.381621</td>\n",
       "      <td>0.308546</td>\n",
       "      <td>0.104502</td>\n",
       "      <td>0.176479</td>\n",
       "      <td>-0.306340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.067224</td>\n",
       "      <td>0.692393</td>\n",
       "      <td>0.748122</td>\n",
       "      <td>-0.282375</td>\n",
       "      <td>0.315609</td>\n",
       "      <td>-0.736757</td>\n",
       "      <td>0.764606</td>\n",
       "      <td>0.204216</td>\n",
       "      <td>-0.237065</td>\n",
       "      <td>-0.218922</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176584</td>\n",
       "      <td>-0.051986</td>\n",
       "      <td>-0.924298</td>\n",
       "      <td>-0.245268</td>\n",
       "      <td>0.241220</td>\n",
       "      <td>-0.908205</td>\n",
       "      <td>-0.617241</td>\n",
       "      <td>-0.393290</td>\n",
       "      <td>0.193927</td>\n",
       "      <td>0.446163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.429942</td>\n",
       "      <td>1.089990</td>\n",
       "      <td>-0.588384</td>\n",
       "      <td>-0.343197</td>\n",
       "      <td>-0.372274</td>\n",
       "      <td>-0.845048</td>\n",
       "      <td>0.918014</td>\n",
       "      <td>0.206775</td>\n",
       "      <td>0.773682</td>\n",
       "      <td>-0.257649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120155</td>\n",
       "      <td>-0.350477</td>\n",
       "      <td>-0.847140</td>\n",
       "      <td>-0.884626</td>\n",
       "      <td>0.518183</td>\n",
       "      <td>-0.339026</td>\n",
       "      <td>-0.122151</td>\n",
       "      <td>-0.600600</td>\n",
       "      <td>0.294227</td>\n",
       "      <td>-0.163380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.188717</td>\n",
       "      <td>0.485727</td>\n",
       "      <td>-0.062433</td>\n",
       "      <td>-0.163184</td>\n",
       "      <td>0.307409</td>\n",
       "      <td>-1.084675</td>\n",
       "      <td>0.887139</td>\n",
       "      <td>0.212792</td>\n",
       "      <td>0.431054</td>\n",
       "      <td>-0.113752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153342</td>\n",
       "      <td>-0.523107</td>\n",
       "      <td>-0.834560</td>\n",
       "      <td>-0.165365</td>\n",
       "      <td>0.639873</td>\n",
       "      <td>-0.978263</td>\n",
       "      <td>-0.390287</td>\n",
       "      <td>-0.713346</td>\n",
       "      <td>0.074052</td>\n",
       "      <td>0.439217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.070405</td>\n",
       "      <td>0.902537</td>\n",
       "      <td>-0.169788</td>\n",
       "      <td>0.030658</td>\n",
       "      <td>-0.075978</td>\n",
       "      <td>-0.769153</td>\n",
       "      <td>1.208868</td>\n",
       "      <td>0.090408</td>\n",
       "      <td>1.002318</td>\n",
       "      <td>0.140205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245980</td>\n",
       "      <td>-0.305059</td>\n",
       "      <td>-0.985106</td>\n",
       "      <td>-0.766145</td>\n",
       "      <td>0.685525</td>\n",
       "      <td>-0.640592</td>\n",
       "      <td>-0.219303</td>\n",
       "      <td>-0.104688</td>\n",
       "      <td>0.340105</td>\n",
       "      <td>0.270191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.340517</td>\n",
       "      <td>0.158451</td>\n",
       "      <td>1.381094</td>\n",
       "      <td>0.215218</td>\n",
       "      <td>-0.466185</td>\n",
       "      <td>0.109660</td>\n",
       "      <td>0.915025</td>\n",
       "      <td>0.478830</td>\n",
       "      <td>0.786674</td>\n",
       "      <td>0.421877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072394</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>-0.499482</td>\n",
       "      <td>-1.300326</td>\n",
       "      <td>-0.267302</td>\n",
       "      <td>-0.461724</td>\n",
       "      <td>0.119142</td>\n",
       "      <td>-0.744191</td>\n",
       "      <td>0.185766</td>\n",
       "      <td>0.430584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.279042</td>\n",
       "      <td>0.256941</td>\n",
       "      <td>-0.173774</td>\n",
       "      <td>0.217073</td>\n",
       "      <td>-0.951093</td>\n",
       "      <td>-0.176427</td>\n",
       "      <td>0.551397</td>\n",
       "      <td>0.576464</td>\n",
       "      <td>0.549970</td>\n",
       "      <td>0.187080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248763</td>\n",
       "      <td>-0.178719</td>\n",
       "      <td>-0.445043</td>\n",
       "      <td>-1.822804</td>\n",
       "      <td>-0.210669</td>\n",
       "      <td>-0.677898</td>\n",
       "      <td>-0.008367</td>\n",
       "      <td>-0.184962</td>\n",
       "      <td>-0.218675</td>\n",
       "      <td>0.240016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.469343</td>\n",
       "      <td>0.249886</td>\n",
       "      <td>0.924985</td>\n",
       "      <td>0.351556</td>\n",
       "      <td>-0.363374</td>\n",
       "      <td>-0.646725</td>\n",
       "      <td>-0.211746</td>\n",
       "      <td>-0.345882</td>\n",
       "      <td>0.034136</td>\n",
       "      <td>-0.022936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022305</td>\n",
       "      <td>-0.312207</td>\n",
       "      <td>-0.482536</td>\n",
       "      <td>-1.538357</td>\n",
       "      <td>0.158994</td>\n",
       "      <td>-0.495581</td>\n",
       "      <td>-0.106075</td>\n",
       "      <td>0.221940</td>\n",
       "      <td>-0.377053</td>\n",
       "      <td>-0.488058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.003576</td>\n",
       "      <td>0.730204</td>\n",
       "      <td>0.829170</td>\n",
       "      <td>-0.384529</td>\n",
       "      <td>0.151655</td>\n",
       "      <td>-0.272048</td>\n",
       "      <td>1.050670</td>\n",
       "      <td>0.415163</td>\n",
       "      <td>-0.329146</td>\n",
       "      <td>-0.237157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147314</td>\n",
       "      <td>-1.076108</td>\n",
       "      <td>-0.753189</td>\n",
       "      <td>0.265270</td>\n",
       "      <td>0.419864</td>\n",
       "      <td>-0.838680</td>\n",
       "      <td>-0.437101</td>\n",
       "      <td>0.056742</td>\n",
       "      <td>0.550285</td>\n",
       "      <td>0.014808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.311714</td>\n",
       "      <td>1.424679</td>\n",
       "      <td>-0.611465</td>\n",
       "      <td>-0.536982</td>\n",
       "      <td>-0.460495</td>\n",
       "      <td>-0.150977</td>\n",
       "      <td>1.210709</td>\n",
       "      <td>0.165027</td>\n",
       "      <td>0.341368</td>\n",
       "      <td>-0.254116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301547</td>\n",
       "      <td>-0.506952</td>\n",
       "      <td>-0.533847</td>\n",
       "      <td>0.106380</td>\n",
       "      <td>-0.006630</td>\n",
       "      <td>-0.868058</td>\n",
       "      <td>0.172977</td>\n",
       "      <td>-0.455419</td>\n",
       "      <td>0.869264</td>\n",
       "      <td>-0.095134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.242460</td>\n",
       "      <td>0.994674</td>\n",
       "      <td>1.671188</td>\n",
       "      <td>0.235587</td>\n",
       "      <td>-0.176122</td>\n",
       "      <td>0.259818</td>\n",
       "      <td>1.321371</td>\n",
       "      <td>0.728926</td>\n",
       "      <td>0.032675</td>\n",
       "      <td>-0.045121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245545</td>\n",
       "      <td>0.054515</td>\n",
       "      <td>0.832415</td>\n",
       "      <td>-0.672990</td>\n",
       "      <td>-0.737821</td>\n",
       "      <td>-0.707966</td>\n",
       "      <td>0.348058</td>\n",
       "      <td>-0.401947</td>\n",
       "      <td>0.586306</td>\n",
       "      <td>0.304912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.150601</td>\n",
       "      <td>0.762169</td>\n",
       "      <td>0.731209</td>\n",
       "      <td>0.050572</td>\n",
       "      <td>0.071322</td>\n",
       "      <td>-0.408579</td>\n",
       "      <td>1.337685</td>\n",
       "      <td>0.095449</td>\n",
       "      <td>0.513310</td>\n",
       "      <td>-0.062508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213121</td>\n",
       "      <td>-0.389607</td>\n",
       "      <td>0.212875</td>\n",
       "      <td>-0.428811</td>\n",
       "      <td>-0.175631</td>\n",
       "      <td>-0.588333</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>-0.290907</td>\n",
       "      <td>-0.186099</td>\n",
       "      <td>0.568507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.752550</td>\n",
       "      <td>-0.602176</td>\n",
       "      <td>-0.255619</td>\n",
       "      <td>-0.081452</td>\n",
       "      <td>-0.253521</td>\n",
       "      <td>1.423792</td>\n",
       "      <td>0.209086</td>\n",
       "      <td>-0.009141</td>\n",
       "      <td>-0.055963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.604650</td>\n",
       "      <td>-0.647825</td>\n",
       "      <td>-1.128386</td>\n",
       "      <td>-0.608950</td>\n",
       "      <td>0.356447</td>\n",
       "      <td>-0.882516</td>\n",
       "      <td>-0.197023</td>\n",
       "      <td>0.685231</td>\n",
       "      <td>0.484927</td>\n",
       "      <td>-0.142602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.035283</td>\n",
       "      <td>1.067358</td>\n",
       "      <td>-0.792664</td>\n",
       "      <td>-0.483826</td>\n",
       "      <td>-0.405407</td>\n",
       "      <td>-0.374977</td>\n",
       "      <td>0.573617</td>\n",
       "      <td>0.106744</td>\n",
       "      <td>0.118057</td>\n",
       "      <td>-0.164380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230914</td>\n",
       "      <td>-1.183574</td>\n",
       "      <td>-0.668581</td>\n",
       "      <td>-0.917211</td>\n",
       "      <td>0.056218</td>\n",
       "      <td>-0.057124</td>\n",
       "      <td>-0.067636</td>\n",
       "      <td>0.808439</td>\n",
       "      <td>-0.074548</td>\n",
       "      <td>-0.512023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.047451</td>\n",
       "      <td>0.118913</td>\n",
       "      <td>0.839871</td>\n",
       "      <td>-0.099999</td>\n",
       "      <td>0.095438</td>\n",
       "      <td>-0.607138</td>\n",
       "      <td>0.250961</td>\n",
       "      <td>0.126526</td>\n",
       "      <td>-0.122442</td>\n",
       "      <td>-0.376142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229134</td>\n",
       "      <td>-0.406150</td>\n",
       "      <td>0.728401</td>\n",
       "      <td>-0.808833</td>\n",
       "      <td>0.080042</td>\n",
       "      <td>-0.394401</td>\n",
       "      <td>-0.406385</td>\n",
       "      <td>0.243626</td>\n",
       "      <td>-0.011293</td>\n",
       "      <td>0.021956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.596883</td>\n",
       "      <td>0.490039</td>\n",
       "      <td>0.395839</td>\n",
       "      <td>-0.280170</td>\n",
       "      <td>-0.143947</td>\n",
       "      <td>0.509206</td>\n",
       "      <td>-0.380598</td>\n",
       "      <td>0.376558</td>\n",
       "      <td>-0.094684</td>\n",
       "      <td>-0.708623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392170</td>\n",
       "      <td>-1.120439</td>\n",
       "      <td>0.292478</td>\n",
       "      <td>0.818811</td>\n",
       "      <td>0.474115</td>\n",
       "      <td>-0.555388</td>\n",
       "      <td>-0.231610</td>\n",
       "      <td>-0.333237</td>\n",
       "      <td>0.539398</td>\n",
       "      <td>-0.287864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.549777</td>\n",
       "      <td>-0.097260</td>\n",
       "      <td>1.597277</td>\n",
       "      <td>0.133152</td>\n",
       "      <td>-0.223687</td>\n",
       "      <td>0.729506</td>\n",
       "      <td>0.397997</td>\n",
       "      <td>0.422940</td>\n",
       "      <td>0.508756</td>\n",
       "      <td>0.091015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118420</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0.784321</td>\n",
       "      <td>-0.075234</td>\n",
       "      <td>-0.459183</td>\n",
       "      <td>-0.140185</td>\n",
       "      <td>0.230550</td>\n",
       "      <td>-0.697934</td>\n",
       "      <td>0.317812</td>\n",
       "      <td>-0.135444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.434719</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>-0.538993</td>\n",
       "      <td>-0.578411</td>\n",
       "      <td>0.301192</td>\n",
       "      <td>-0.482952</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.114858</td>\n",
       "      <td>0.815766</td>\n",
       "      <td>-0.053669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>-0.451492</td>\n",
       "      <td>-0.585941</td>\n",
       "      <td>0.557998</td>\n",
       "      <td>0.664056</td>\n",
       "      <td>-0.511697</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>-0.597898</td>\n",
       "      <td>0.555260</td>\n",
       "      <td>-0.028660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.594598</td>\n",
       "      <td>0.521112</td>\n",
       "      <td>-1.161313</td>\n",
       "      <td>-0.248543</td>\n",
       "      <td>0.100609</td>\n",
       "      <td>-0.973035</td>\n",
       "      <td>0.123046</td>\n",
       "      <td>0.490730</td>\n",
       "      <td>0.604955</td>\n",
       "      <td>-0.472792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413437</td>\n",
       "      <td>-0.960970</td>\n",
       "      <td>-0.633244</td>\n",
       "      <td>0.934605</td>\n",
       "      <td>0.876308</td>\n",
       "      <td>-0.044396</td>\n",
       "      <td>-0.551465</td>\n",
       "      <td>-0.684952</td>\n",
       "      <td>0.685084</td>\n",
       "      <td>-0.468441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>-0.429979</td>\n",
       "      <td>0.923306</td>\n",
       "      <td>-0.398015</td>\n",
       "      <td>0.188991</td>\n",
       "      <td>-0.263006</td>\n",
       "      <td>-0.782631</td>\n",
       "      <td>0.657560</td>\n",
       "      <td>0.162837</td>\n",
       "      <td>-0.490767</td>\n",
       "      <td>0.417126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.681405</td>\n",
       "      <td>0.328603</td>\n",
       "      <td>-0.829605</td>\n",
       "      <td>-1.682948</td>\n",
       "      <td>0.598860</td>\n",
       "      <td>-1.108177</td>\n",
       "      <td>-0.116733</td>\n",
       "      <td>0.053739</td>\n",
       "      <td>-0.422095</td>\n",
       "      <td>0.260725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15143</th>\n",
       "      <td>-0.043578</td>\n",
       "      <td>0.555581</td>\n",
       "      <td>-0.099086</td>\n",
       "      <td>0.390959</td>\n",
       "      <td>-0.225849</td>\n",
       "      <td>0.353738</td>\n",
       "      <td>1.284665</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>-0.137463</td>\n",
       "      <td>-0.115161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386830</td>\n",
       "      <td>0.401434</td>\n",
       "      <td>-0.407920</td>\n",
       "      <td>-1.314025</td>\n",
       "      <td>0.026493</td>\n",
       "      <td>-0.500112</td>\n",
       "      <td>-0.543099</td>\n",
       "      <td>0.492693</td>\n",
       "      <td>-1.051776</td>\n",
       "      <td>0.030858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15144</th>\n",
       "      <td>-0.135589</td>\n",
       "      <td>-0.743354</td>\n",
       "      <td>2.667785</td>\n",
       "      <td>0.666865</td>\n",
       "      <td>0.407194</td>\n",
       "      <td>0.247118</td>\n",
       "      <td>-0.461257</td>\n",
       "      <td>1.051878</td>\n",
       "      <td>0.648655</td>\n",
       "      <td>-0.452914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337445</td>\n",
       "      <td>-1.074436</td>\n",
       "      <td>0.473650</td>\n",
       "      <td>-0.384414</td>\n",
       "      <td>-0.606001</td>\n",
       "      <td>-0.199056</td>\n",
       "      <td>-0.275823</td>\n",
       "      <td>-0.339070</td>\n",
       "      <td>-0.273938</td>\n",
       "      <td>-0.065053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15145</th>\n",
       "      <td>-0.648439</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>1.437418</td>\n",
       "      <td>0.534736</td>\n",
       "      <td>0.809205</td>\n",
       "      <td>-0.408701</td>\n",
       "      <td>-0.889373</td>\n",
       "      <td>0.082069</td>\n",
       "      <td>-0.678171</td>\n",
       "      <td>-0.627719</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033761</td>\n",
       "      <td>-1.226396</td>\n",
       "      <td>-0.710445</td>\n",
       "      <td>-0.316640</td>\n",
       "      <td>0.723148</td>\n",
       "      <td>-0.431823</td>\n",
       "      <td>-0.426380</td>\n",
       "      <td>0.242747</td>\n",
       "      <td>0.055056</td>\n",
       "      <td>-0.130414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15146</th>\n",
       "      <td>0.270864</td>\n",
       "      <td>0.635750</td>\n",
       "      <td>-1.087887</td>\n",
       "      <td>0.584023</td>\n",
       "      <td>0.699082</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.582099</td>\n",
       "      <td>0.219382</td>\n",
       "      <td>-0.073709</td>\n",
       "      <td>0.424745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.680093</td>\n",
       "      <td>0.262663</td>\n",
       "      <td>-1.142458</td>\n",
       "      <td>0.042301</td>\n",
       "      <td>0.366019</td>\n",
       "      <td>-0.275833</td>\n",
       "      <td>0.406303</td>\n",
       "      <td>-0.050831</td>\n",
       "      <td>0.257194</td>\n",
       "      <td>0.056717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15147</th>\n",
       "      <td>0.312779</td>\n",
       "      <td>0.540572</td>\n",
       "      <td>-0.785549</td>\n",
       "      <td>0.251227</td>\n",
       "      <td>0.499297</td>\n",
       "      <td>-0.151244</td>\n",
       "      <td>0.478609</td>\n",
       "      <td>0.162228</td>\n",
       "      <td>-0.041705</td>\n",
       "      <td>0.310991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465182</td>\n",
       "      <td>0.063255</td>\n",
       "      <td>-1.463951</td>\n",
       "      <td>0.214107</td>\n",
       "      <td>0.276807</td>\n",
       "      <td>-0.300008</td>\n",
       "      <td>0.338580</td>\n",
       "      <td>-0.114483</td>\n",
       "      <td>0.259812</td>\n",
       "      <td>-0.084181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15148</th>\n",
       "      <td>-0.158719</td>\n",
       "      <td>0.550882</td>\n",
       "      <td>1.464439</td>\n",
       "      <td>0.539607</td>\n",
       "      <td>0.815227</td>\n",
       "      <td>-0.191627</td>\n",
       "      <td>0.569211</td>\n",
       "      <td>-0.396422</td>\n",
       "      <td>-0.109373</td>\n",
       "      <td>-0.800955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294250</td>\n",
       "      <td>-0.671359</td>\n",
       "      <td>-0.976365</td>\n",
       "      <td>0.145848</td>\n",
       "      <td>0.056655</td>\n",
       "      <td>-0.837044</td>\n",
       "      <td>-0.349773</td>\n",
       "      <td>-0.714834</td>\n",
       "      <td>-0.476675</td>\n",
       "      <td>-0.166453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15149</th>\n",
       "      <td>-0.382025</td>\n",
       "      <td>0.925010</td>\n",
       "      <td>-1.672172</td>\n",
       "      <td>0.336933</td>\n",
       "      <td>0.376965</td>\n",
       "      <td>0.564326</td>\n",
       "      <td>-0.055448</td>\n",
       "      <td>0.155457</td>\n",
       "      <td>-0.144216</td>\n",
       "      <td>0.374998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452419</td>\n",
       "      <td>-0.278073</td>\n",
       "      <td>-1.446044</td>\n",
       "      <td>0.588079</td>\n",
       "      <td>0.207245</td>\n",
       "      <td>-0.949150</td>\n",
       "      <td>-0.031177</td>\n",
       "      <td>-0.063503</td>\n",
       "      <td>0.361659</td>\n",
       "      <td>-0.021994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15150</th>\n",
       "      <td>0.381812</td>\n",
       "      <td>0.523448</td>\n",
       "      <td>0.792268</td>\n",
       "      <td>0.303277</td>\n",
       "      <td>0.420096</td>\n",
       "      <td>-0.645599</td>\n",
       "      <td>0.297162</td>\n",
       "      <td>-0.099895</td>\n",
       "      <td>-0.025503</td>\n",
       "      <td>-0.734172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007991</td>\n",
       "      <td>-0.428729</td>\n",
       "      <td>-0.794829</td>\n",
       "      <td>-1.003780</td>\n",
       "      <td>0.670536</td>\n",
       "      <td>-1.016085</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>-0.091946</td>\n",
       "      <td>-0.010824</td>\n",
       "      <td>-0.219707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15151</th>\n",
       "      <td>-0.383282</td>\n",
       "      <td>0.859473</td>\n",
       "      <td>-1.045189</td>\n",
       "      <td>0.464266</td>\n",
       "      <td>0.399274</td>\n",
       "      <td>-0.379001</td>\n",
       "      <td>-0.145227</td>\n",
       "      <td>-0.094983</td>\n",
       "      <td>-0.161760</td>\n",
       "      <td>-0.137406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.815088</td>\n",
       "      <td>-0.256754</td>\n",
       "      <td>-1.562443</td>\n",
       "      <td>-0.033242</td>\n",
       "      <td>0.375652</td>\n",
       "      <td>-1.144490</td>\n",
       "      <td>-0.337582</td>\n",
       "      <td>0.287632</td>\n",
       "      <td>0.239830</td>\n",
       "      <td>-0.166706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15152</th>\n",
       "      <td>-0.340742</td>\n",
       "      <td>-0.514651</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>0.361260</td>\n",
       "      <td>0.207014</td>\n",
       "      <td>-0.140352</td>\n",
       "      <td>0.780601</td>\n",
       "      <td>-0.438325</td>\n",
       "      <td>-0.333867</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.375923</td>\n",
       "      <td>-0.324809</td>\n",
       "      <td>-0.342801</td>\n",
       "      <td>-0.673059</td>\n",
       "      <td>0.143284</td>\n",
       "      <td>-1.069113</td>\n",
       "      <td>0.074865</td>\n",
       "      <td>0.442380</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>-0.164789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15153</th>\n",
       "      <td>0.644566</td>\n",
       "      <td>0.089218</td>\n",
       "      <td>1.865409</td>\n",
       "      <td>-0.296457</td>\n",
       "      <td>0.367235</td>\n",
       "      <td>0.755234</td>\n",
       "      <td>1.226127</td>\n",
       "      <td>0.682475</td>\n",
       "      <td>-0.211964</td>\n",
       "      <td>0.588518</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034301</td>\n",
       "      <td>0.403738</td>\n",
       "      <td>1.048541</td>\n",
       "      <td>-0.098952</td>\n",
       "      <td>-0.460521</td>\n",
       "      <td>-0.885755</td>\n",
       "      <td>-0.461025</td>\n",
       "      <td>0.641683</td>\n",
       "      <td>-0.520082</td>\n",
       "      <td>-0.742827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15154</th>\n",
       "      <td>-0.191982</td>\n",
       "      <td>0.235138</td>\n",
       "      <td>0.595061</td>\n",
       "      <td>0.189478</td>\n",
       "      <td>0.234170</td>\n",
       "      <td>-0.699225</td>\n",
       "      <td>-0.552261</td>\n",
       "      <td>-0.812654</td>\n",
       "      <td>-0.398165</td>\n",
       "      <td>-0.202596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.353726</td>\n",
       "      <td>0.083256</td>\n",
       "      <td>-0.747245</td>\n",
       "      <td>-0.163717</td>\n",
       "      <td>0.356386</td>\n",
       "      <td>0.030085</td>\n",
       "      <td>0.176072</td>\n",
       "      <td>-1.001497</td>\n",
       "      <td>-0.263090</td>\n",
       "      <td>0.151105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15155</th>\n",
       "      <td>-0.060122</td>\n",
       "      <td>0.701429</td>\n",
       "      <td>-0.078798</td>\n",
       "      <td>-0.102878</td>\n",
       "      <td>0.055221</td>\n",
       "      <td>-0.780194</td>\n",
       "      <td>0.707146</td>\n",
       "      <td>-0.777344</td>\n",
       "      <td>0.184682</td>\n",
       "      <td>-0.513885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.881174</td>\n",
       "      <td>-1.148170</td>\n",
       "      <td>-1.567004</td>\n",
       "      <td>-0.917136</td>\n",
       "      <td>0.479223</td>\n",
       "      <td>-0.773991</td>\n",
       "      <td>-0.214615</td>\n",
       "      <td>-0.371144</td>\n",
       "      <td>-0.577864</td>\n",
       "      <td>-0.020228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15156</th>\n",
       "      <td>-0.067975</td>\n",
       "      <td>0.533574</td>\n",
       "      <td>0.071156</td>\n",
       "      <td>0.248730</td>\n",
       "      <td>0.153657</td>\n",
       "      <td>-0.838445</td>\n",
       "      <td>-0.363410</td>\n",
       "      <td>-0.900107</td>\n",
       "      <td>-0.398769</td>\n",
       "      <td>-0.229365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.483118</td>\n",
       "      <td>-0.162320</td>\n",
       "      <td>-0.850727</td>\n",
       "      <td>-0.332967</td>\n",
       "      <td>0.625880</td>\n",
       "      <td>-0.178130</td>\n",
       "      <td>0.147170</td>\n",
       "      <td>-0.589899</td>\n",
       "      <td>0.097675</td>\n",
       "      <td>0.041570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15157</th>\n",
       "      <td>-0.226983</td>\n",
       "      <td>0.229725</td>\n",
       "      <td>1.617918</td>\n",
       "      <td>0.158010</td>\n",
       "      <td>0.115952</td>\n",
       "      <td>-1.211147</td>\n",
       "      <td>0.269561</td>\n",
       "      <td>-0.413930</td>\n",
       "      <td>-0.424964</td>\n",
       "      <td>-0.584629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104633</td>\n",
       "      <td>-0.450451</td>\n",
       "      <td>-1.069046</td>\n",
       "      <td>0.217021</td>\n",
       "      <td>0.346621</td>\n",
       "      <td>-0.537835</td>\n",
       "      <td>-0.573620</td>\n",
       "      <td>-0.537656</td>\n",
       "      <td>0.465864</td>\n",
       "      <td>0.348023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15158</th>\n",
       "      <td>0.134859</td>\n",
       "      <td>0.737623</td>\n",
       "      <td>0.371777</td>\n",
       "      <td>0.090902</td>\n",
       "      <td>-0.011337</td>\n",
       "      <td>-0.758235</td>\n",
       "      <td>0.353526</td>\n",
       "      <td>-1.112746</td>\n",
       "      <td>0.549447</td>\n",
       "      <td>0.197746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.398194</td>\n",
       "      <td>-0.211936</td>\n",
       "      <td>-0.888031</td>\n",
       "      <td>-0.744577</td>\n",
       "      <td>0.210855</td>\n",
       "      <td>-0.363124</td>\n",
       "      <td>0.031836</td>\n",
       "      <td>-0.777193</td>\n",
       "      <td>-0.720292</td>\n",
       "      <td>-0.024841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15159</th>\n",
       "      <td>0.380124</td>\n",
       "      <td>-0.584867</td>\n",
       "      <td>2.128113</td>\n",
       "      <td>0.174499</td>\n",
       "      <td>0.107736</td>\n",
       "      <td>1.289057</td>\n",
       "      <td>-0.443420</td>\n",
       "      <td>0.791467</td>\n",
       "      <td>0.014391</td>\n",
       "      <td>-0.337498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856129</td>\n",
       "      <td>0.746297</td>\n",
       "      <td>0.480660</td>\n",
       "      <td>-0.057790</td>\n",
       "      <td>0.109012</td>\n",
       "      <td>-0.361681</td>\n",
       "      <td>0.171841</td>\n",
       "      <td>0.319443</td>\n",
       "      <td>0.540761</td>\n",
       "      <td>-0.100416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15160</th>\n",
       "      <td>-0.438783</td>\n",
       "      <td>0.526718</td>\n",
       "      <td>-0.260171</td>\n",
       "      <td>-0.067837</td>\n",
       "      <td>0.111793</td>\n",
       "      <td>-1.169548</td>\n",
       "      <td>0.725577</td>\n",
       "      <td>-0.323997</td>\n",
       "      <td>0.404917</td>\n",
       "      <td>-0.843509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459796</td>\n",
       "      <td>-0.595449</td>\n",
       "      <td>-1.033490</td>\n",
       "      <td>-1.090840</td>\n",
       "      <td>0.757682</td>\n",
       "      <td>-0.659478</td>\n",
       "      <td>0.090856</td>\n",
       "      <td>-0.640813</td>\n",
       "      <td>0.180680</td>\n",
       "      <td>0.035249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15161</th>\n",
       "      <td>0.095803</td>\n",
       "      <td>0.105755</td>\n",
       "      <td>0.748500</td>\n",
       "      <td>-0.047110</td>\n",
       "      <td>0.474164</td>\n",
       "      <td>-0.839090</td>\n",
       "      <td>0.868266</td>\n",
       "      <td>-0.701754</td>\n",
       "      <td>0.165768</td>\n",
       "      <td>-0.255399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.565084</td>\n",
       "      <td>-1.034593</td>\n",
       "      <td>-1.428221</td>\n",
       "      <td>-0.996250</td>\n",
       "      <td>0.119550</td>\n",
       "      <td>-1.047976</td>\n",
       "      <td>-0.229160</td>\n",
       "      <td>-0.079709</td>\n",
       "      <td>-0.558403</td>\n",
       "      <td>0.368162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15162</th>\n",
       "      <td>-0.764210</td>\n",
       "      <td>0.020937</td>\n",
       "      <td>0.948614</td>\n",
       "      <td>0.478805</td>\n",
       "      <td>-0.112949</td>\n",
       "      <td>-1.477446</td>\n",
       "      <td>-0.384555</td>\n",
       "      <td>0.259454</td>\n",
       "      <td>0.339522</td>\n",
       "      <td>-0.354813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292252</td>\n",
       "      <td>-0.829976</td>\n",
       "      <td>0.283592</td>\n",
       "      <td>-1.917113</td>\n",
       "      <td>-0.135760</td>\n",
       "      <td>-0.213594</td>\n",
       "      <td>-0.100686</td>\n",
       "      <td>-0.170573</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>0.045480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15163</th>\n",
       "      <td>0.029231</td>\n",
       "      <td>0.159379</td>\n",
       "      <td>1.627265</td>\n",
       "      <td>0.299543</td>\n",
       "      <td>0.489727</td>\n",
       "      <td>-1.472448</td>\n",
       "      <td>-0.268372</td>\n",
       "      <td>0.881339</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>-0.640878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.489847</td>\n",
       "      <td>-0.483856</td>\n",
       "      <td>-1.348406</td>\n",
       "      <td>0.191891</td>\n",
       "      <td>0.174963</td>\n",
       "      <td>-0.637120</td>\n",
       "      <td>-0.788922</td>\n",
       "      <td>-0.076925</td>\n",
       "      <td>0.616618</td>\n",
       "      <td>0.056276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15164</th>\n",
       "      <td>0.069879</td>\n",
       "      <td>0.654006</td>\n",
       "      <td>1.040663</td>\n",
       "      <td>0.253406</td>\n",
       "      <td>0.375032</td>\n",
       "      <td>-0.363762</td>\n",
       "      <td>-0.086726</td>\n",
       "      <td>0.548025</td>\n",
       "      <td>-0.195549</td>\n",
       "      <td>-0.077331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246732</td>\n",
       "      <td>-1.122616</td>\n",
       "      <td>-0.969490</td>\n",
       "      <td>-0.340310</td>\n",
       "      <td>-0.159195</td>\n",
       "      <td>0.023582</td>\n",
       "      <td>-0.697915</td>\n",
       "      <td>0.289685</td>\n",
       "      <td>-0.386796</td>\n",
       "      <td>0.298517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15165</th>\n",
       "      <td>-0.419999</td>\n",
       "      <td>-0.243872</td>\n",
       "      <td>1.203773</td>\n",
       "      <td>0.565788</td>\n",
       "      <td>0.234181</td>\n",
       "      <td>-0.732491</td>\n",
       "      <td>-0.388387</td>\n",
       "      <td>0.818503</td>\n",
       "      <td>-0.108605</td>\n",
       "      <td>0.074084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.528034</td>\n",
       "      <td>-0.850408</td>\n",
       "      <td>0.956272</td>\n",
       "      <td>0.184463</td>\n",
       "      <td>-0.467964</td>\n",
       "      <td>-0.748257</td>\n",
       "      <td>-0.237885</td>\n",
       "      <td>-0.090730</td>\n",
       "      <td>-0.074381</td>\n",
       "      <td>0.509266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15166</th>\n",
       "      <td>-0.523914</td>\n",
       "      <td>0.198489</td>\n",
       "      <td>0.150241</td>\n",
       "      <td>0.104944</td>\n",
       "      <td>0.230465</td>\n",
       "      <td>-0.366978</td>\n",
       "      <td>0.472738</td>\n",
       "      <td>-1.339408</td>\n",
       "      <td>0.191624</td>\n",
       "      <td>-0.232928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900349</td>\n",
       "      <td>-0.637291</td>\n",
       "      <td>-0.632091</td>\n",
       "      <td>-0.850005</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>-0.503389</td>\n",
       "      <td>-0.298256</td>\n",
       "      <td>0.072940</td>\n",
       "      <td>-0.983077</td>\n",
       "      <td>0.007896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15167</th>\n",
       "      <td>-0.256953</td>\n",
       "      <td>0.648578</td>\n",
       "      <td>-0.140227</td>\n",
       "      <td>-0.040917</td>\n",
       "      <td>0.198057</td>\n",
       "      <td>-0.730375</td>\n",
       "      <td>0.445954</td>\n",
       "      <td>-0.885738</td>\n",
       "      <td>0.013102</td>\n",
       "      <td>-0.517669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916832</td>\n",
       "      <td>-1.096182</td>\n",
       "      <td>-1.719435</td>\n",
       "      <td>-0.854368</td>\n",
       "      <td>0.627329</td>\n",
       "      <td>-0.834519</td>\n",
       "      <td>-0.204094</td>\n",
       "      <td>-0.525800</td>\n",
       "      <td>-0.467193</td>\n",
       "      <td>-0.123517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15168</th>\n",
       "      <td>-0.208698</td>\n",
       "      <td>0.379544</td>\n",
       "      <td>1.554406</td>\n",
       "      <td>0.124116</td>\n",
       "      <td>-0.086512</td>\n",
       "      <td>-1.091874</td>\n",
       "      <td>0.807734</td>\n",
       "      <td>-0.530874</td>\n",
       "      <td>0.493482</td>\n",
       "      <td>-0.841219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731390</td>\n",
       "      <td>-0.725609</td>\n",
       "      <td>-0.254688</td>\n",
       "      <td>-1.686912</td>\n",
       "      <td>0.115246</td>\n",
       "      <td>-1.095707</td>\n",
       "      <td>-0.437711</td>\n",
       "      <td>-0.194225</td>\n",
       "      <td>-1.100700</td>\n",
       "      <td>0.167775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15169</th>\n",
       "      <td>0.405978</td>\n",
       "      <td>0.567655</td>\n",
       "      <td>0.745709</td>\n",
       "      <td>0.089495</td>\n",
       "      <td>-0.829722</td>\n",
       "      <td>-0.950194</td>\n",
       "      <td>0.472688</td>\n",
       "      <td>-0.747963</td>\n",
       "      <td>0.869636</td>\n",
       "      <td>-0.003402</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.033980</td>\n",
       "      <td>-1.064792</td>\n",
       "      <td>-0.764211</td>\n",
       "      <td>-1.887691</td>\n",
       "      <td>0.034109</td>\n",
       "      <td>-0.600403</td>\n",
       "      <td>-0.187079</td>\n",
       "      <td>-0.505494</td>\n",
       "      <td>-0.619860</td>\n",
       "      <td>0.062247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15170</th>\n",
       "      <td>0.028926</td>\n",
       "      <td>0.637616</td>\n",
       "      <td>0.754861</td>\n",
       "      <td>0.045799</td>\n",
       "      <td>-0.283006</td>\n",
       "      <td>-1.193167</td>\n",
       "      <td>0.687876</td>\n",
       "      <td>-0.675943</td>\n",
       "      <td>0.430138</td>\n",
       "      <td>-0.382108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765781</td>\n",
       "      <td>-0.456621</td>\n",
       "      <td>-1.033800</td>\n",
       "      <td>-1.645679</td>\n",
       "      <td>0.256671</td>\n",
       "      <td>-0.630034</td>\n",
       "      <td>0.014068</td>\n",
       "      <td>-0.905407</td>\n",
       "      <td>-0.364500</td>\n",
       "      <td>-0.122163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15171</th>\n",
       "      <td>0.031907</td>\n",
       "      <td>0.477019</td>\n",
       "      <td>1.425380</td>\n",
       "      <td>-0.048335</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>-1.109246</td>\n",
       "      <td>1.019129</td>\n",
       "      <td>-0.538573</td>\n",
       "      <td>0.369773</td>\n",
       "      <td>-1.161486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097518</td>\n",
       "      <td>-0.509644</td>\n",
       "      <td>-0.629713</td>\n",
       "      <td>-0.761676</td>\n",
       "      <td>-0.187932</td>\n",
       "      <td>-0.548651</td>\n",
       "      <td>-0.189356</td>\n",
       "      <td>-0.806427</td>\n",
       "      <td>-0.643233</td>\n",
       "      <td>0.422549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15172 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.164120 -0.019730  0.203762 -0.149297  0.696591 -0.960957  0.206785   \n",
       "1     -0.107122  0.944672 -0.452394 -0.229796 -0.732791 -0.794121  1.180954   \n",
       "2      0.123370  0.182647  0.767236 -0.386340  0.239815 -1.276435  0.194789   \n",
       "3     -0.212266  0.199948  1.093049 -0.079362  0.368825 -0.464936  0.845249   \n",
       "4     -0.082211  0.736390  0.620443  0.304812  0.169626 -0.476543  0.072635   \n",
       "5     -0.504248  0.465273  0.330162 -0.282355  0.195496 -1.244920  0.398042   \n",
       "6     -0.228612  0.261782 -0.260801 -0.214646  0.547143 -0.754278  0.029804   \n",
       "7     -0.090555  0.529084  0.356458 -0.081107  0.355607 -0.283721  0.824368   \n",
       "8      0.010008  0.962858 -0.633553 -0.451542 -0.093861 -1.025709  0.891725   \n",
       "9     -0.131621  0.445831 -0.713069 -0.268536 -0.454286 -0.289466  0.878650   \n",
       "10     0.291645  0.150018  0.474314  0.366117  0.266019 -0.231297  0.071569   \n",
       "11     0.598658  0.033788  2.198835 -0.107228  0.247022 -0.053831 -0.721075   \n",
       "12     0.067224  0.692393  0.748122 -0.282375  0.315609 -0.736757  0.764606   \n",
       "13    -0.429942  1.089990 -0.588384 -0.343197 -0.372274 -0.845048  0.918014   \n",
       "14    -0.188717  0.485727 -0.062433 -0.163184  0.307409 -1.084675  0.887139   \n",
       "15     0.070405  0.902537 -0.169788  0.030658 -0.075978 -0.769153  1.208868   \n",
       "16     0.340517  0.158451  1.381094  0.215218 -0.466185  0.109660  0.915025   \n",
       "17    -0.279042  0.256941 -0.173774  0.217073 -0.951093 -0.176427  0.551397   \n",
       "18    -0.469343  0.249886  0.924985  0.351556 -0.363374 -0.646725 -0.211746   \n",
       "19    -0.003576  0.730204  0.829170 -0.384529  0.151655 -0.272048  1.050670   \n",
       "20     0.311714  1.424679 -0.611465 -0.536982 -0.460495 -0.150977  1.210709   \n",
       "21     0.242460  0.994674  1.671188  0.235587 -0.176122  0.259818  1.321371   \n",
       "22    -0.150601  0.762169  0.731209  0.050572  0.071322 -0.408579  1.337685   \n",
       "23    -0.004294  0.752550 -0.602176 -0.255619 -0.081452 -0.253521  1.423792   \n",
       "24     0.035283  1.067358 -0.792664 -0.483826 -0.405407 -0.374977  0.573617   \n",
       "25     0.047451  0.118913  0.839871 -0.099999  0.095438 -0.607138  0.250961   \n",
       "26     0.596883  0.490039  0.395839 -0.280170 -0.143947  0.509206 -0.380598   \n",
       "27     0.549777 -0.097260  1.597277  0.133152 -0.223687  0.729506  0.397997   \n",
       "28     0.434719  0.969423 -0.538993 -0.578411  0.301192 -0.482952  0.090030   \n",
       "29     0.594598  0.521112 -1.161313 -0.248543  0.100609 -0.973035  0.123046   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "15142 -0.429979  0.923306 -0.398015  0.188991 -0.263006 -0.782631  0.657560   \n",
       "15143 -0.043578  0.555581 -0.099086  0.390959 -0.225849  0.353738  1.284665   \n",
       "15144 -0.135589 -0.743354  2.667785  0.666865  0.407194  0.247118 -0.461257   \n",
       "15145 -0.648439  0.020761  1.437418  0.534736  0.809205 -0.408701 -0.889373   \n",
       "15146  0.270864  0.635750 -1.087887  0.584023  0.699082  0.002700  0.582099   \n",
       "15147  0.312779  0.540572 -0.785549  0.251227  0.499297 -0.151244  0.478609   \n",
       "15148 -0.158719  0.550882  1.464439  0.539607  0.815227 -0.191627  0.569211   \n",
       "15149 -0.382025  0.925010 -1.672172  0.336933  0.376965  0.564326 -0.055448   \n",
       "15150  0.381812  0.523448  0.792268  0.303277  0.420096 -0.645599  0.297162   \n",
       "15151 -0.383282  0.859473 -1.045189  0.464266  0.399274 -0.379001 -0.145227   \n",
       "15152 -0.340742 -0.514651  0.058269  0.361260  0.207014 -0.140352  0.780601   \n",
       "15153  0.644566  0.089218  1.865409 -0.296457  0.367235  0.755234  1.226127   \n",
       "15154 -0.191982  0.235138  0.595061  0.189478  0.234170 -0.699225 -0.552261   \n",
       "15155 -0.060122  0.701429 -0.078798 -0.102878  0.055221 -0.780194  0.707146   \n",
       "15156 -0.067975  0.533574  0.071156  0.248730  0.153657 -0.838445 -0.363410   \n",
       "15157 -0.226983  0.229725  1.617918  0.158010  0.115952 -1.211147  0.269561   \n",
       "15158  0.134859  0.737623  0.371777  0.090902 -0.011337 -0.758235  0.353526   \n",
       "15159  0.380124 -0.584867  2.128113  0.174499  0.107736  1.289057 -0.443420   \n",
       "15160 -0.438783  0.526718 -0.260171 -0.067837  0.111793 -1.169548  0.725577   \n",
       "15161  0.095803  0.105755  0.748500 -0.047110  0.474164 -0.839090  0.868266   \n",
       "15162 -0.764210  0.020937  0.948614  0.478805 -0.112949 -1.477446 -0.384555   \n",
       "15163  0.029231  0.159379  1.627265  0.299543  0.489727 -1.472448 -0.268372   \n",
       "15164  0.069879  0.654006  1.040663  0.253406  0.375032 -0.363762 -0.086726   \n",
       "15165 -0.419999 -0.243872  1.203773  0.565788  0.234181 -0.732491 -0.388387   \n",
       "15166 -0.523914  0.198489  0.150241  0.104944  0.230465 -0.366978  0.472738   \n",
       "15167 -0.256953  0.648578 -0.140227 -0.040917  0.198057 -0.730375  0.445954   \n",
       "15168 -0.208698  0.379544  1.554406  0.124116 -0.086512 -1.091874  0.807734   \n",
       "15169  0.405978  0.567655  0.745709  0.089495 -0.829722 -0.950194  0.472688   \n",
       "15170  0.028926  0.637616  0.754861  0.045799 -0.283006 -1.193167  0.687876   \n",
       "15171  0.031907  0.477019  1.425380 -0.048335  0.001622 -1.109246  1.019129   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "0     -0.540251  0.005157  0.196422  ... -0.075324 -0.620497 -0.303472   \n",
       "1     -0.231252  0.270980  0.060620  ... -0.069693  0.726365 -0.519457   \n",
       "2     -0.089553  0.052065 -0.120395  ... -0.506447 -1.584028 -0.943574   \n",
       "3      0.332470 -0.713169 -0.262536  ... -0.256164 -1.225924 -1.114888   \n",
       "4      0.401900 -0.055084  0.132189  ... -0.188720  0.061769  0.530789   \n",
       "5      0.102070 -0.213497  0.194332  ... -0.177531 -0.336883 -0.527773   \n",
       "6     -0.378770 -0.459770 -0.506010  ... -0.112834 -0.710977 -0.672821   \n",
       "7      0.386737 -0.593540 -0.308293  ... -0.310062 -0.642304 -1.027118   \n",
       "8      0.094641  0.341399  0.166679  ... -0.415344 -0.358977 -1.049884   \n",
       "9      0.204928 -0.058836  0.309174  ... -0.374667 -0.532018 -0.597064   \n",
       "10     0.231684 -0.013936  0.295219  ... -0.179121 -0.125835  0.485977   \n",
       "11     0.855839 -0.312170 -0.006771  ...  0.285903 -0.690131  0.990308   \n",
       "12     0.204216 -0.237065 -0.218922  ... -0.176584 -0.051986 -0.924298   \n",
       "13     0.206775  0.773682 -0.257649  ... -0.120155 -0.350477 -0.847140   \n",
       "14     0.212792  0.431054 -0.113752  ... -0.153342 -0.523107 -0.834560   \n",
       "15     0.090408  1.002318  0.140205  ...  0.245980 -0.305059 -0.985106   \n",
       "16     0.478830  0.786674  0.421877  ...  0.072394  0.005762 -0.499482   \n",
       "17     0.576464  0.549970  0.187080  ... -0.248763 -0.178719 -0.445043   \n",
       "18    -0.345882  0.034136 -0.022936  ... -0.022305 -0.312207 -0.482536   \n",
       "19     0.415163 -0.329146 -0.237157  ...  0.147314 -1.076108 -0.753189   \n",
       "20     0.165027  0.341368 -0.254116  ...  0.301547 -0.506952 -0.533847   \n",
       "21     0.728926  0.032675 -0.045121  ... -0.245545  0.054515  0.832415   \n",
       "22     0.095449  0.513310 -0.062508  ... -0.213121 -0.389607  0.212875   \n",
       "23     0.209086 -0.009141 -0.055963  ... -0.604650 -0.647825 -1.128386   \n",
       "24     0.106744  0.118057 -0.164380  ...  0.230914 -1.183574 -0.668581   \n",
       "25     0.126526 -0.122442 -0.376142  ...  0.229134 -0.406150  0.728401   \n",
       "26     0.376558 -0.094684 -0.708623  ...  0.392170 -1.120439  0.292478   \n",
       "27     0.422940  0.508756  0.091015  ...  0.118420  0.168930  0.784321   \n",
       "28     0.114858  0.815766 -0.053669  ...  0.002846 -0.451492 -0.585941   \n",
       "29     0.490730  0.604955 -0.472792  ...  0.413437 -0.960970 -0.633244   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "15142  0.162837 -0.490767  0.417126  ... -0.681405  0.328603 -0.829605   \n",
       "15143  0.006688 -0.137463 -0.115161  ... -0.386830  0.401434 -0.407920   \n",
       "15144  1.051878  0.648655 -0.452914  ...  0.337445 -1.074436  0.473650   \n",
       "15145  0.082069 -0.678171 -0.627719  ... -0.033761 -1.226396 -0.710445   \n",
       "15146  0.219382 -0.073709  0.424745  ... -0.680093  0.262663 -1.142458   \n",
       "15147  0.162228 -0.041705  0.310991  ... -0.465182  0.063255 -1.463951   \n",
       "15148 -0.396422 -0.109373 -0.800955  ... -0.294250 -0.671359 -0.976365   \n",
       "15149  0.155457 -0.144216  0.374998  ... -0.452419 -0.278073 -1.446044   \n",
       "15150 -0.099895 -0.025503 -0.734172  ... -0.007991 -0.428729 -0.794829   \n",
       "15151 -0.094983 -0.161760 -0.137406  ... -0.815088 -0.256754 -1.562443   \n",
       "15152 -0.438325 -0.333867  0.384058  ... -0.375923 -0.324809 -0.342801   \n",
       "15153  0.682475 -0.211964  0.588518  ... -0.034301  0.403738  1.048541   \n",
       "15154 -0.812654 -0.398165 -0.202596  ... -0.353726  0.083256 -0.747245   \n",
       "15155 -0.777344  0.184682 -0.513885  ... -0.881174 -1.148170 -1.567004   \n",
       "15156 -0.900107 -0.398769 -0.229365  ... -0.483118 -0.162320 -0.850727   \n",
       "15157 -0.413930 -0.424964 -0.584629  ... -0.104633 -0.450451 -1.069046   \n",
       "15158 -1.112746  0.549447  0.197746  ... -0.398194 -0.211936 -0.888031   \n",
       "15159  0.791467  0.014391 -0.337498  ...  0.856129  0.746297  0.480660   \n",
       "15160 -0.323997  0.404917 -0.843509  ... -0.459796 -0.595449 -1.033490   \n",
       "15161 -0.701754  0.165768 -0.255399  ... -0.565084 -1.034593 -1.428221   \n",
       "15162  0.259454  0.339522 -0.354813  ... -0.292252 -0.829976  0.283592   \n",
       "15163  0.881339 -0.148438 -0.640878  ... -0.489847 -0.483856 -1.348406   \n",
       "15164  0.548025 -0.195549 -0.077331  ...  0.246732 -1.122616 -0.969490   \n",
       "15165  0.818503 -0.108605  0.074084  ... -0.528034 -0.850408  0.956272   \n",
       "15166 -1.339408  0.191624 -0.232928  ... -0.900349 -0.637291 -0.632091   \n",
       "15167 -0.885738  0.013102 -0.517669  ... -0.916832 -1.096182 -1.719435   \n",
       "15168 -0.530874  0.493482 -0.841219  ... -0.731390 -0.725609 -0.254688   \n",
       "15169 -0.747963  0.869636 -0.003402  ... -1.033980 -1.064792 -0.764211   \n",
       "15170 -0.675943  0.430138 -0.382108  ... -0.765781 -0.456621 -1.033800   \n",
       "15171 -0.538573  0.369773 -1.161486  ... -0.097518 -0.509644 -0.629713   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "0      0.297969  1.019830 -0.523595 -0.525541  0.337973  0.422184 -0.103847  \n",
       "1     -1.590454  0.438942 -0.663646  0.016005 -0.209250 -0.010170  0.619601  \n",
       "2     -1.807983  0.863276 -0.925124 -0.041773  0.223409 -0.194369 -0.310801  \n",
       "3     -0.392965  0.123237 -0.763239 -0.119133  0.222245  0.562734  0.133977  \n",
       "4     -1.138050 -0.434086  0.134580  0.068409 -0.174906  0.013871  0.111412  \n",
       "5      0.365958  0.612401 -0.599148 -0.148439 -0.568717  0.717854  0.218804  \n",
       "6      0.589150  0.225079 -0.839100  0.156029  0.297599  1.139234 -0.182229  \n",
       "7      0.195643  0.058157 -0.368076 -0.401911  0.302515  0.576539  0.144626  \n",
       "8      0.403800  0.391501 -0.288238 -0.054236 -0.175665  1.020589  0.036927  \n",
       "9     -0.810273  0.433057 -0.209578  0.001626 -0.561577 -0.110241 -0.092543  \n",
       "10    -0.688165 -0.111847 -0.687745 -0.606426  0.098401 -0.434292  0.285049  \n",
       "11    -0.683531  0.219738 -0.381621  0.308546  0.104502  0.176479 -0.306340  \n",
       "12    -0.245268  0.241220 -0.908205 -0.617241 -0.393290  0.193927  0.446163  \n",
       "13    -0.884626  0.518183 -0.339026 -0.122151 -0.600600  0.294227 -0.163380  \n",
       "14    -0.165365  0.639873 -0.978263 -0.390287 -0.713346  0.074052  0.439217  \n",
       "15    -0.766145  0.685525 -0.640592 -0.219303 -0.104688  0.340105  0.270191  \n",
       "16    -1.300326 -0.267302 -0.461724  0.119142 -0.744191  0.185766  0.430584  \n",
       "17    -1.822804 -0.210669 -0.677898 -0.008367 -0.184962 -0.218675  0.240016  \n",
       "18    -1.538357  0.158994 -0.495581 -0.106075  0.221940 -0.377053 -0.488058  \n",
       "19     0.265270  0.419864 -0.838680 -0.437101  0.056742  0.550285  0.014808  \n",
       "20     0.106380 -0.006630 -0.868058  0.172977 -0.455419  0.869264 -0.095134  \n",
       "21    -0.672990 -0.737821 -0.707966  0.348058 -0.401947  0.586306  0.304912  \n",
       "22    -0.428811 -0.175631 -0.588333  0.008460 -0.290907 -0.186099  0.568507  \n",
       "23    -0.608950  0.356447 -0.882516 -0.197023  0.685231  0.484927 -0.142602  \n",
       "24    -0.917211  0.056218 -0.057124 -0.067636  0.808439 -0.074548 -0.512023  \n",
       "25    -0.808833  0.080042 -0.394401 -0.406385  0.243626 -0.011293  0.021956  \n",
       "26     0.818811  0.474115 -0.555388 -0.231610 -0.333237  0.539398 -0.287864  \n",
       "27    -0.075234 -0.459183 -0.140185  0.230550 -0.697934  0.317812 -0.135444  \n",
       "28     0.557998  0.664056 -0.511697  0.070064 -0.597898  0.555260 -0.028660  \n",
       "29     0.934605  0.876308 -0.044396 -0.551465 -0.684952  0.685084 -0.468441  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "15142 -1.682948  0.598860 -1.108177 -0.116733  0.053739 -0.422095  0.260725  \n",
       "15143 -1.314025  0.026493 -0.500112 -0.543099  0.492693 -1.051776  0.030858  \n",
       "15144 -0.384414 -0.606001 -0.199056 -0.275823 -0.339070 -0.273938 -0.065053  \n",
       "15145 -0.316640  0.723148 -0.431823 -0.426380  0.242747  0.055056 -0.130414  \n",
       "15146  0.042301  0.366019 -0.275833  0.406303 -0.050831  0.257194  0.056717  \n",
       "15147  0.214107  0.276807 -0.300008  0.338580 -0.114483  0.259812 -0.084181  \n",
       "15148  0.145848  0.056655 -0.837044 -0.349773 -0.714834 -0.476675 -0.166453  \n",
       "15149  0.588079  0.207245 -0.949150 -0.031177 -0.063503  0.361659 -0.021994  \n",
       "15150 -1.003780  0.670536 -1.016085  0.018160 -0.091946 -0.010824 -0.219707  \n",
       "15151 -0.033242  0.375652 -1.144490 -0.337582  0.287632  0.239830 -0.166706  \n",
       "15152 -0.673059  0.143284 -1.069113  0.074865  0.442380  0.010725 -0.164789  \n",
       "15153 -0.098952 -0.460521 -0.885755 -0.461025  0.641683 -0.520082 -0.742827  \n",
       "15154 -0.163717  0.356386  0.030085  0.176072 -1.001497 -0.263090  0.151105  \n",
       "15155 -0.917136  0.479223 -0.773991 -0.214615 -0.371144 -0.577864 -0.020228  \n",
       "15156 -0.332967  0.625880 -0.178130  0.147170 -0.589899  0.097675  0.041570  \n",
       "15157  0.217021  0.346621 -0.537835 -0.573620 -0.537656  0.465864  0.348023  \n",
       "15158 -0.744577  0.210855 -0.363124  0.031836 -0.777193 -0.720292 -0.024841  \n",
       "15159 -0.057790  0.109012 -0.361681  0.171841  0.319443  0.540761 -0.100416  \n",
       "15160 -1.090840  0.757682 -0.659478  0.090856 -0.640813  0.180680  0.035249  \n",
       "15161 -0.996250  0.119550 -1.047976 -0.229160 -0.079709 -0.558403  0.368162  \n",
       "15162 -1.917113 -0.135760 -0.213594 -0.100686 -0.170573  0.007234  0.045480  \n",
       "15163  0.191891  0.174963 -0.637120 -0.788922 -0.076925  0.616618  0.056276  \n",
       "15164 -0.340310 -0.159195  0.023582 -0.697915  0.289685 -0.386796  0.298517  \n",
       "15165  0.184463 -0.467964 -0.748257 -0.237885 -0.090730 -0.074381  0.509266  \n",
       "15166 -0.850005  0.249668 -0.503389 -0.298256  0.072940 -0.983077  0.007896  \n",
       "15167 -0.854368  0.627329 -0.834519 -0.204094 -0.525800 -0.467193 -0.123517  \n",
       "15168 -1.686912  0.115246 -1.095707 -0.437711 -0.194225 -1.100700  0.167775  \n",
       "15169 -1.887691  0.034109 -0.600403 -0.187079 -0.505494 -0.619860  0.062247  \n",
       "15170 -1.645679  0.256671 -0.630034  0.014068 -0.905407 -0.364500 -0.122163  \n",
       "15171 -0.761676 -0.187932 -0.548651 -0.189356 -0.806427 -0.643233  0.422549  \n",
       "\n",
       "[15172 rows x 768 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_embeds = X_embeds[list(X_embeds.columns[11:779])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_embeds, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANFORMERS FOR PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "tfidf = TfidfVectorizer(tokenizer = tokenize_sentence, \n",
    "                               min_df=5, max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_embeds_train = X_train[list(X_train.columns[11:779])]\n",
    "just_embeds_test = X_test[list(X_test.columns[11:779])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train['tfidf']=tfidf.fit_transform(X_train['text'])\n",
    "X_test['tfidf']=tfidf.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tfidf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tfidf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-aceb9edefcd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tfidf'"
     ]
    }
   ],
   "source": [
    "train_tfidf = pd.DataFrame(X_train['tfidf'].values.tolist())\n",
    "test_tfidf = pd.DataFrame(X_test['tfidf'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "column_tuples_train = [\n",
    "    ('text', tfidf),\n",
    "    ('sentiment_score', None),\n",
    "    ('abs_sent_score', None),\n",
    "    ('punct_count', None),\n",
    "    ('word_count', None),\n",
    "    ('%adj', None),\n",
    "    ('%verb', None),\n",
    "    ('%adv', None), \n",
    "    ('%noun', None),\n",
    "    ('avg_word_length', None),\n",
    "    ('strong_subjectives_count', None)\n",
    "]\n",
    "\n",
    "\n",
    "mapper_1 = DataFrameMapper(column_tuples_train, df_out=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_mapped = mapper_1.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mapped = mapper_1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = X_train_mapped.join(just_embeds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all = X_test_mapped.join(just_embeds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all[['sentiment_score','abs_sent_score','punct_count',\n",
    "        'word_count','%adj','%verb','%adv','%noun',\n",
    "        'avg_word_length',\n",
    "         'strong_subjectives_count']] = scaler.fit_transform(X_train_[['sentiment_score',\n",
    "                                        'abs_sent_score','punct_count',\n",
    "                                       'word_count','%adj','%verb','%adv','%noun',\n",
    "                                       'avg_word_length','strong_subjectives_count']])\n",
    "X_test_all[['sentiment_score','abs_sent_score','punct_count',\n",
    "        'word_count','%adj','%verb','%adv','%noun',\n",
    "        'avg_word_length','strong_subjectives_count']]=scaler.transform(X_test[[\n",
    "                                        'sentiment_score','abs_sent_score','punct_count',\n",
    "                                       'word_count','%adj','%verb','%adv','%noun',\n",
    "                                       'avg_word_length','strong_subjectives_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>strong_subjectives_count</th>\n",
       "      <th>embeds</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13001</th>\n",
       "      <td>0.378942</td>\n",
       "      <td>-0.584117</td>\n",
       "      <td>0.107380</td>\n",
       "      <td>1.787735</td>\n",
       "      <td>0.125383</td>\n",
       "      <td>0.155285</td>\n",
       "      <td>-0.298495</td>\n",
       "      <td>0.849950</td>\n",
       "      <td>-0.302474</td>\n",
       "      <td>-0.814549</td>\n",
       "      <td>[-0.17333102, 0.8431093, 0.81538683, -0.283583...</td>\n",
       "      <td>(0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13937</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-0.409388</td>\n",
       "      <td>0.314773</td>\n",
       "      <td>0.438876</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.792049</td>\n",
       "      <td>-0.648888</td>\n",
       "      <td>0.585855</td>\n",
       "      <td>[0.0918742, -0.13311641, -0.01790656, 0.670809...</td>\n",
       "      <td>(0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12521</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>0.517110</td>\n",
       "      <td>-0.182099</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.552504</td>\n",
       "      <td>-0.080325</td>\n",
       "      <td>-0.273210</td>\n",
       "      <td>0.703670</td>\n",
       "      <td>0.585855</td>\n",
       "      <td>[-0.16667514, -0.09049596, 0.25222436, 0.00745...</td>\n",
       "      <td>(0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9363</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>1.336569</td>\n",
       "      <td>0.045189</td>\n",
       "      <td>-0.277755</td>\n",
       "      <td>-0.448369</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.398204</td>\n",
       "      <td>0.460798</td>\n",
       "      <td>-0.814549</td>\n",
       "      <td>[-0.31779048, 0.6162055, 1.2091062, 0.37545416...</td>\n",
       "      <td>(0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14796</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-1.091253</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.830264</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.370299</td>\n",
       "      <td>-0.448944</td>\n",
       "      <td>-0.814549</td>\n",
       "      <td>[0.52177453, 0.31714925, 0.9615935, 0.6633711,...</td>\n",
       "      <td>(0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment_score  abs_sent_score  punct_count  word_count      %adj  \\\n",
       "13001         0.378942       -0.584117     0.107380    1.787735  0.125383   \n",
       "13937         0.069278       -1.026921    -0.712079   -0.409388  0.314773   \n",
       "12521         0.069278       -1.026921     0.517110   -0.182099 -0.861440   \n",
       "9363          0.069278       -1.026921     1.336569    0.045189 -0.277755   \n",
       "14796         0.069278       -1.026921    -0.712079   -1.091253 -0.861440   \n",
       "\n",
       "          %verb      %adv     %noun  avg_word_length  \\\n",
       "13001  0.155285 -0.298495  0.849950        -0.302474   \n",
       "13937  0.438876 -0.540039  0.792049        -0.648888   \n",
       "12521  0.552504 -0.080325 -0.273210         0.703670   \n",
       "9363  -0.448369 -0.540039  0.398204         0.460798   \n",
       "14796  0.830264 -0.540039  0.370299        -0.448944   \n",
       "\n",
       "       strong_subjectives_count  \\\n",
       "13001                 -0.814549   \n",
       "13937                  0.585855   \n",
       "12521                  0.585855   \n",
       "9363                  -0.814549   \n",
       "14796                 -0.814549   \n",
       "\n",
       "                                                  embeds  \\\n",
       "13001  [-0.17333102, 0.8431093, 0.81538683, -0.283583...   \n",
       "13937  [0.0918742, -0.13311641, -0.01790656, 0.670809...   \n",
       "12521  [-0.16667514, -0.09049596, 0.25222436, 0.00745...   \n",
       "9363   [-0.31779048, 0.6162055, 1.2091062, 0.37545416...   \n",
       "14796  [0.52177453, 0.31714925, 0.9615935, 0.6633711,...   \n",
       "\n",
       "                                                   tfidf  \n",
       "13001    (0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...  \n",
       "13937    (0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...  \n",
       "12521    (0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...  \n",
       "9363     (0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...  \n",
       "14796    (0, 3226)\\t0.16293961526714112\\n  (0, 2811)\\...  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all.to_csv('X_train_all_transformed.csv')\n",
    "X_test_all.to_csv('X_test_all_transformed.csv')\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv')\n",
    "pd.DataFrame(y_test).to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_abandon</th>\n",
       "      <th>text_abedin</th>\n",
       "      <th>text_abid</th>\n",
       "      <th>text_ability</th>\n",
       "      <th>text_able</th>\n",
       "      <th>text_abnormal</th>\n",
       "      <th>text_abortion</th>\n",
       "      <th>text_abraham</th>\n",
       "      <th>text_absolute</th>\n",
       "      <th>text_absolutely</th>\n",
       "      <th>...</th>\n",
       "      <th>text_york</th>\n",
       "      <th>text_yorker</th>\n",
       "      <th>text_youare</th>\n",
       "      <th>text_young</th>\n",
       "      <th>text_youth</th>\n",
       "      <th>text_zakkout</th>\n",
       "      <th>text_zarate</th>\n",
       "      <th>text_zero</th>\n",
       "      <th>text_zionist</th>\n",
       "      <th>text_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13937</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12521</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.394301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_abandon  text_abedin  text_abid  text_ability  text_able  \\\n",
       "13001           0.0          0.0        0.0           0.0        0.0   \n",
       "13937           0.0          0.0        0.0           0.0        0.0   \n",
       "12521           0.0          0.0        0.0           0.0        0.0   \n",
       "9363            0.0          0.0        0.0           0.0        0.0   \n",
       "14796           0.0          0.0        0.0           0.0        0.0   \n",
       "\n",
       "       text_abnormal  text_abortion  text_abraham  text_absolute  \\\n",
       "13001            0.0            0.0           0.0            0.0   \n",
       "13937            0.0            0.0           0.0            0.0   \n",
       "12521            0.0            0.0           0.0            0.0   \n",
       "9363             0.0            0.0           0.0            0.0   \n",
       "14796            0.0            0.0           0.0            0.0   \n",
       "\n",
       "       text_absolutely  ...  text_york  text_yorker  text_youare  text_young  \\\n",
       "13001              0.0  ...        0.0     0.000000          0.0         0.0   \n",
       "13937              0.0  ...        0.0     0.000000          0.0         0.0   \n",
       "12521              0.0  ...        0.0     0.394301          0.0         0.0   \n",
       "9363               0.0  ...        0.0     0.000000          0.0         0.0   \n",
       "14796              0.0  ...        0.0     0.000000          0.0         0.0   \n",
       "\n",
       "       text_youth  text_zakkout  text_zarate  text_zero  text_zionist  \\\n",
       "13001         0.0           0.0          0.0        0.0           0.0   \n",
       "13937         0.0           0.0          0.0        0.0           0.0   \n",
       "12521         0.0           0.0          0.0        0.0           0.0   \n",
       "9363          0.0           0.0          0.0        0.0           0.0   \n",
       "14796         0.0           0.0          0.0        0.0           0.0   \n",
       "\n",
       "       text_zone  \n",
       "13001        0.0  \n",
       "13937        0.0  \n",
       "12521        0.0  \n",
       "9363         0.0  \n",
       "14796        0.0  \n",
       "\n",
       "[5 rows x 3262 columns]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all[X_train_all.columns[0:3262]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>strong_subjectives_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13001</th>\n",
       "      <td>0.378942</td>\n",
       "      <td>-0.584117</td>\n",
       "      <td>0.107380</td>\n",
       "      <td>1.787735</td>\n",
       "      <td>0.125383</td>\n",
       "      <td>0.155285</td>\n",
       "      <td>-0.298495</td>\n",
       "      <td>0.849950</td>\n",
       "      <td>-0.302474</td>\n",
       "      <td>-0.814549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13937</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-0.409388</td>\n",
       "      <td>0.314773</td>\n",
       "      <td>0.438876</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.792049</td>\n",
       "      <td>-0.648888</td>\n",
       "      <td>0.585855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12521</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>0.517110</td>\n",
       "      <td>-0.182099</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.552504</td>\n",
       "      <td>-0.080325</td>\n",
       "      <td>-0.273210</td>\n",
       "      <td>0.703670</td>\n",
       "      <td>0.585855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9363</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>1.336569</td>\n",
       "      <td>0.045189</td>\n",
       "      <td>-0.277755</td>\n",
       "      <td>-0.448369</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.398204</td>\n",
       "      <td>0.460798</td>\n",
       "      <td>-0.814549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14796</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-1.091253</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.830264</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.370299</td>\n",
       "      <td>-0.448944</td>\n",
       "      <td>-0.814549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment_score  abs_sent_score  punct_count  word_count      %adj  \\\n",
       "13001         0.378942       -0.584117     0.107380    1.787735  0.125383   \n",
       "13937         0.069278       -1.026921    -0.712079   -0.409388  0.314773   \n",
       "12521         0.069278       -1.026921     0.517110   -0.182099 -0.861440   \n",
       "9363          0.069278       -1.026921     1.336569    0.045189 -0.277755   \n",
       "14796         0.069278       -1.026921    -0.712079   -1.091253 -0.861440   \n",
       "\n",
       "          %verb      %adv     %noun  avg_word_length  strong_subjectives_count  \n",
       "13001  0.155285 -0.298495  0.849950        -0.302474                 -0.814549  \n",
       "13937  0.438876 -0.540039  0.792049        -0.648888                  0.585855  \n",
       "12521  0.552504 -0.080325 -0.273210         0.703670                  0.585855  \n",
       "9363  -0.448369 -0.540039  0.398204         0.460798                 -0.814549  \n",
       "14796  0.830264 -0.540039  0.370299        -0.448944                 -0.814549  "
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all[X_train_all.columns[3262:3272]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13001</th>\n",
       "      <td>-0.173331</td>\n",
       "      <td>0.843109</td>\n",
       "      <td>0.815387</td>\n",
       "      <td>-0.283583</td>\n",
       "      <td>0.355310</td>\n",
       "      <td>-0.363973</td>\n",
       "      <td>1.265644</td>\n",
       "      <td>-0.608121</td>\n",
       "      <td>-0.231724</td>\n",
       "      <td>-0.349867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251001</td>\n",
       "      <td>-1.381743</td>\n",
       "      <td>-0.772514</td>\n",
       "      <td>-0.686197</td>\n",
       "      <td>-0.173441</td>\n",
       "      <td>-0.560432</td>\n",
       "      <td>-0.213605</td>\n",
       "      <td>0.026848</td>\n",
       "      <td>-0.679761</td>\n",
       "      <td>0.458733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13937</th>\n",
       "      <td>0.091874</td>\n",
       "      <td>-0.133116</td>\n",
       "      <td>-0.017907</td>\n",
       "      <td>0.670810</td>\n",
       "      <td>0.950990</td>\n",
       "      <td>0.213923</td>\n",
       "      <td>0.284865</td>\n",
       "      <td>-0.094425</td>\n",
       "      <td>0.490145</td>\n",
       "      <td>0.336642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335247</td>\n",
       "      <td>-0.472793</td>\n",
       "      <td>-0.247906</td>\n",
       "      <td>-1.039255</td>\n",
       "      <td>-0.247866</td>\n",
       "      <td>-0.295682</td>\n",
       "      <td>0.041536</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.719729</td>\n",
       "      <td>0.212103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12521</th>\n",
       "      <td>-0.166675</td>\n",
       "      <td>-0.090496</td>\n",
       "      <td>0.252224</td>\n",
       "      <td>0.007459</td>\n",
       "      <td>0.778086</td>\n",
       "      <td>0.036884</td>\n",
       "      <td>0.177676</td>\n",
       "      <td>0.318603</td>\n",
       "      <td>0.459911</td>\n",
       "      <td>0.622604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215225</td>\n",
       "      <td>-0.969704</td>\n",
       "      <td>-0.959864</td>\n",
       "      <td>-0.327255</td>\n",
       "      <td>-0.499393</td>\n",
       "      <td>-0.895054</td>\n",
       "      <td>-0.787237</td>\n",
       "      <td>-0.379003</td>\n",
       "      <td>-0.772760</td>\n",
       "      <td>-0.057005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9363</th>\n",
       "      <td>-0.317790</td>\n",
       "      <td>0.616206</td>\n",
       "      <td>1.209106</td>\n",
       "      <td>0.375454</td>\n",
       "      <td>0.138737</td>\n",
       "      <td>0.572094</td>\n",
       "      <td>0.818017</td>\n",
       "      <td>-0.115805</td>\n",
       "      <td>0.259089</td>\n",
       "      <td>0.281828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351264</td>\n",
       "      <td>0.681804</td>\n",
       "      <td>0.360461</td>\n",
       "      <td>-1.479023</td>\n",
       "      <td>-0.932557</td>\n",
       "      <td>0.317828</td>\n",
       "      <td>0.135406</td>\n",
       "      <td>-0.529601</td>\n",
       "      <td>-0.902091</td>\n",
       "      <td>-0.090235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14796</th>\n",
       "      <td>0.521775</td>\n",
       "      <td>0.317149</td>\n",
       "      <td>0.961594</td>\n",
       "      <td>0.663371</td>\n",
       "      <td>-0.138175</td>\n",
       "      <td>-0.537674</td>\n",
       "      <td>0.436469</td>\n",
       "      <td>1.175056</td>\n",
       "      <td>0.449023</td>\n",
       "      <td>0.549446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.331839</td>\n",
       "      <td>0.055938</td>\n",
       "      <td>1.047838</td>\n",
       "      <td>-1.078855</td>\n",
       "      <td>-0.295900</td>\n",
       "      <td>-0.543152</td>\n",
       "      <td>-0.308911</td>\n",
       "      <td>-0.110881</td>\n",
       "      <td>0.041825</td>\n",
       "      <td>0.191739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "13001 -0.173331  0.843109  0.815387 -0.283583  0.355310 -0.363973  1.265644   \n",
       "13937  0.091874 -0.133116 -0.017907  0.670810  0.950990  0.213923  0.284865   \n",
       "12521 -0.166675 -0.090496  0.252224  0.007459  0.778086  0.036884  0.177676   \n",
       "9363  -0.317790  0.616206  1.209106  0.375454  0.138737  0.572094  0.818017   \n",
       "14796  0.521775  0.317149  0.961594  0.663371 -0.138175 -0.537674  0.436469   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "13001 -0.608121 -0.231724 -0.349867  ... -0.251001 -1.381743 -0.772514   \n",
       "13937 -0.094425  0.490145  0.336642  ...  0.335247 -0.472793 -0.247906   \n",
       "12521  0.318603  0.459911  0.622604  ... -0.215225 -0.969704 -0.959864   \n",
       "9363  -0.115805  0.259089  0.281828  ...  0.351264  0.681804  0.360461   \n",
       "14796  1.175056  0.449023  0.549446  ... -0.331839  0.055938  1.047838   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "13001 -0.686197 -0.173441 -0.560432 -0.213605  0.026848 -0.679761  0.458733  \n",
       "13937 -1.039255 -0.247866 -0.295682  0.041536 -0.012181 -0.719729  0.212103  \n",
       "12521 -0.327255 -0.499393 -0.895054 -0.787237 -0.379003 -0.772760 -0.057005  \n",
       "9363  -1.479023 -0.932557  0.317828  0.135406 -0.529601 -0.902091 -0.090235  \n",
       "14796 -1.078855 -0.295900 -0.543152 -0.308911 -0.110881  0.041825  0.191739  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all[X_train_all.columns[3272:4040]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Hyper-Parameters Taken From Optimizing Each Model with Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_embeds = LogisticRegression(C = 3, penalty = 'l2', solver='newton-cg', \n",
    "                                    class_weight = 'balanced', max_iter = 1000)\n",
    "\n",
    "log_reg_tfidf = LogisticRegression(penalty = 'l2', C = 29.763514416313132, \n",
    "                                   solver = 'lbfgs', class_weight = 'balanced', max_iter = 1000)\n",
    "\n",
    "log_reg_meta = LogisticRegression(penalty = 'l2', C = 0.012742749857031334, \n",
    "                                   solver = 'sag', class_weight = 'balanced', max_iter = 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "hyperparam_grid_logistic = {'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'class_weight': 'balanced'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf_log = RandomizedSearchCV(logistic, hyperparam_grid_logistic, random_state=1, n_iter=200, cv=5, \n",
    "                         verbose=True, n_jobs=-1, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_cols = [i for i in X_train_all.columns[3272:4040]]\n",
    "tfidf_cols = [str(i) for i in X_train_all.columns[0:3262]]\n",
    "meta_cols = [i for i in X_train_all.columns[3262:3272]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def get_embeds_cols(df):\n",
    "    return df[embeds_cols]\n",
    "\n",
    "def get_tfidf_cols(df):\n",
    "    return df[tfidf_cols]\n",
    "\n",
    "def get_meta_cols(df):\n",
    "    return df[meta_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_embeds = make_pipeline(FunctionTransformer(get_embeds_cols, validate=False), log_reg_embeds)\n",
    "pipe_tfidf = make_pipeline(FunctionTransformer(get_tfidf_cols, validate=False), log_reg_tfidf)\n",
    "pipe_meta = make_pipeline(FunctionTransformer(get_meta_cols, validate=False), log_reg_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf = StackingClassifier(estimators=[('pipe_embeds', pipe_embeds), ('pipe_tfidf', pipe_tfidf), \n",
    "                                      ('pipe_meta', pipe_meta)], final_estimator=clf_log,\n",
    "                          cv=5,stack_method='decision_function', n_jobs=-1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2 = LogisticRegression(C = 3, penalty = 'l2', solver='newton-cg', \n",
    "                                    class_weight = 'balanced', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf2 = StackingClassifier(estimators=[('pipe_embeds', pipe_embeds), ('pipe_tfidf', pipe_tfidf), \n",
    "                                      ('pipe_meta', pipe_meta)], final_estimator=clf_2,\n",
    "                          cv=5,stack_method='decision_function', n_jobs=-1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=5,\n",
       "                   estimators=[('pipe_embeds',\n",
       "                                Pipeline(memory=None,\n",
       "                                         steps=[('functiontransformer',\n",
       "                                                 FunctionTransformer(accept_sparse=False,\n",
       "                                                                     check_inverse=True,\n",
       "                                                                     func=<function get_embeds_cols at 0x1a1a060d90>,\n",
       "                                                                     inv_kw_args=None,\n",
       "                                                                     inverse_func=None,\n",
       "                                                                     kw_args=None,\n",
       "                                                                     validate=False)),\n",
       "                                                ('logisticregression',\n",
       "                                                 LogisticRegression(C=3,\n",
       "                                                                    class_weight='balanced',\n",
       "                                                                    dual=False...\n",
       "                                         verbose=False))],\n",
       "                   final_estimator=LogisticRegression(C=3,\n",
       "                                                      class_weight='balanced',\n",
       "                                                      dual=False,\n",
       "                                                      fit_intercept=True,\n",
       "                                                      intercept_scaling=1,\n",
       "                                                      l1_ratio=None,\n",
       "                                                      max_iter=1000,\n",
       "                                                      multi_class='auto',\n",
       "                                                      n_jobs=None, penalty='l2',\n",
       "                                                      random_state=None,\n",
       "                                                      solver='newton-cg',\n",
       "                                                      tol=0.0001, verbose=0,\n",
       "                                                      warm_start=False),\n",
       "                   n_jobs=-1, passthrough=False,\n",
       "                   stack_method='decision_function', verbose=True)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf2.fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target vector\n",
    "stacked_preds_2 = sclf2.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1294 2216]\n",
      " [ 127 1370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.911     0.369     0.525      3510\n",
      "           1      0.382     0.915     0.539      1497\n",
      "\n",
      "    accuracy                          0.532      5007\n",
      "   macro avg      0.646     0.642     0.532      5007\n",
      "weighted avg      0.753     0.532     0.529      5007\n",
      "\n",
      "0.6419123146578056\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, stacked_preds_2))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, stacked_preds_2, digits=3))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test, stacked_preds_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    9.7s finished\n"
     ]
    }
   ],
   "source": [
    "best_stacked_model = sclf.fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 5,\n",
       " 'error_score': nan,\n",
       " 'estimator__C': 1.0,\n",
       " 'estimator__class_weight': None,\n",
       " 'estimator__dual': False,\n",
       " 'estimator__fit_intercept': True,\n",
       " 'estimator__intercept_scaling': 1,\n",
       " 'estimator__l1_ratio': None,\n",
       " 'estimator__max_iter': 100,\n",
       " 'estimator__multi_class': 'auto',\n",
       " 'estimator__n_jobs': None,\n",
       " 'estimator__penalty': 'l2',\n",
       " 'estimator__random_state': None,\n",
       " 'estimator__solver': 'lbfgs',\n",
       " 'estimator__tol': 0.0001,\n",
       " 'estimator__verbose': 0,\n",
       " 'estimator__warm_start': False,\n",
       " 'estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'iid': 'deprecated',\n",
       " 'n_iter': 200,\n",
       " 'n_jobs': -1,\n",
       " 'param_distributions': {'penalty': ['l1', 'l2'],\n",
       "  'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "         4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "         2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "         1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "         5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "  'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
       "  'class_weight': 'balanced'},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'random_state': 1,\n",
       " 'refit': True,\n",
       " 'return_train_score': False,\n",
       " 'scoring': 'f1',\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.final_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # View best hyperparameters\n",
    "# print('Best Penalty:', sclf.final_estimator_.get_params()['penalty'])\n",
    "# print('Best C:', sclf.final_estimator_.get_params()['C'])\n",
    "# print('Best solver:', sclf.final_estimator_.get_params()['solver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target vector\n",
    "stacked_preds = best_stacked_model.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2257 1253]\n",
      " [ 387 1110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.854     0.643     0.734      3510\n",
      "           1      0.470     0.741     0.575      1497\n",
      "\n",
      "    accuracy                          0.672      5007\n",
      "   macro avg      0.662     0.692     0.654      5007\n",
      "weighted avg      0.739     0.672     0.686      5007\n",
      "\n",
      "0.6922514544759033\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, stacked_preds))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, stacked_preds, digits=3))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test, stacked_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function get_embeds_cols at 0x1a1a0609d8>: it's not the same object as __main__.get_embeds_cols",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-457-900763180c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Save the trained model as a pickle string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msaved_stacked_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_stacked_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the pickled model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function get_embeds_cols at 0x1a1a0609d8>: it's not the same object as __main__.get_embeds_cols"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "  \n",
    "# Save the trained model as a pickle string. \n",
    "saved_stacked_model = pickle.dumps(best_stacked_model) \n",
    "  \n",
    "# Load the pickled model \n",
    "stacked_from_pickle = pickle.loads(saved_stacked_model) \n",
    "  \n",
    "# Use the loaded pickled model to make predictions \n",
    "stacked_from_pickle.predict(X_test_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_preds.to_csv('stacked_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA output shape: (10165, 500)\n",
      "Sum of explained variance ratio: 98%\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=500, n_iter = 50)\n",
    "x_train_svd = svd.fit_transform(X_train_all)\n",
    "x_test_svd = svd.transform(X_test_all)\n",
    "print('LSA output shape:', x_train_svd.shape)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "hyperparam_grid_logistic = {'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'class_weight': 'balanced'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf_log_all = RandomizedSearchCV(logistic, hyperparam_grid_logistic, random_state=1, n_iter=1000, cv=5, \n",
    "                         verbose=True, n_jobs=-1, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 23.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 36.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 52.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 72.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 97.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 121.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 153.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed: 153.1min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit randomized search\n",
    "best_model_log_all = clf_log_all.fit(x_train_svd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 0.012742749857031334\n",
      "Best solver: newton-cg\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model_log_all.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model_log_all.best_estimator_.get_params()['C'])\n",
    "print('Best solver:', best_model_log_all.best_estimator_.get_params()['solver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target vector\n",
    "log_all_preds = best_model_log_all.predict(x_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 949 2561]\n",
      " [ 101 1396]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.904     0.270     0.416      3510\n",
      "           1      0.353     0.933     0.512      1497\n",
      "\n",
      "    accuracy                          0.468      5007\n",
      "   macro avg      0.628     0.601     0.464      5007\n",
      "weighted avg      0.739     0.468     0.445      5007\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6014510502486454"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, log_all_preds))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, log_all_preds, digits=3))\n",
    "\n",
    "roc_auc_score(y_test, log_all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
