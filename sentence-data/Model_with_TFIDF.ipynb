{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from pydotplus import graph_from_dot_data\n",
    "import pandas\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #imports\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "# import spacy \n",
    "# import en_core_web_sm\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lang.en import English\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentence_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>propaganda</th>\n",
       "      <th>propaganda_type</th>\n",
       "      <th>text</th>\n",
       "      <th>prop_txt_snippet</th>\n",
       "      <th>sent_#</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Florida Muslim Leader Sofian Zakkout’s D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>701225819</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Name_Calling,Labeling</td>\n",
       "      <td>David Duke, the white supremacist icon and for...</td>\n",
       "      <td>Grand Wizard of the Ku Klux Klan</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>4.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>701225819</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>However, one individual who represents the Mus...</td>\n",
       "      <td>enamored</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Last month, once again, Zakkout chose to showc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>5.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The postings can be rivaled only by Zakkout’s ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>4.636364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id      propaganda        propaganda_type  \\\n",
       "0   701225819  non-propaganda                    NaN   \n",
       "1   701225819      propaganda  Name_Calling,Labeling   \n",
       "2   701225819      propaganda        Loaded_Language   \n",
       "3   701225819  non-propaganda                    NaN   \n",
       "4   701225819  non-propaganda                    NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  South Florida Muslim Leader Sofian Zakkout’s D...   \n",
       "1  David Duke, the white supremacist icon and for...   \n",
       "2  However, one individual who represents the Mus...   \n",
       "3  Last month, once again, Zakkout chose to showc...   \n",
       "4  The postings can be rivaled only by Zakkout’s ...   \n",
       "\n",
       "                   prop_txt_snippet  sent_#  sentiment_score  abs_sent_score  \\\n",
       "0                               NaN       1           0.0000          0.0000   \n",
       "1  Grand Wizard of the Ku Klux Klan       2           0.5423          0.5423   \n",
       "2                          enamored       3           0.3612          0.3612   \n",
       "3                               NaN       4           0.0000          0.0000   \n",
       "4                               NaN       5           0.0000          0.0000   \n",
       "\n",
       "   punct_count  word_count      %adj     %verb      %adv     %noun  \\\n",
       "0            0           9  0.000000  0.000000  0.000000  0.000000   \n",
       "1            4          26  0.020548  0.006849  0.013699  0.006849   \n",
       "2            4          27  0.017241  0.017241  0.005747  0.022989   \n",
       "3            5          22  0.021127  0.021127  0.014085  0.035211   \n",
       "4            1          11  0.014493  0.043478  0.014493  0.028986   \n",
       "\n",
       "   avg_word_length  \n",
       "0         5.444444  \n",
       "1         4.423077  \n",
       "2         5.000000  \n",
       "3         5.045455  \n",
       "4         4.636364  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = df.drop(['article_id','propaganda_type','prop_txt_snippet','sent_#'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15220 entries, 0 to 15219\n",
      "Data columns (total 11 columns):\n",
      "propaganda         15220 non-null object\n",
      "text               15220 non-null object\n",
      "sentiment_score    15220 non-null float64\n",
      "abs_sent_score     15220 non-null float64\n",
      "punct_count        15220 non-null int64\n",
      "word_count         15220 non-null int64\n",
      "%adj               15220 non-null float64\n",
      "%verb              15220 non-null float64\n",
      "%adv               15220 non-null float64\n",
      "%noun              15220 non-null float64\n",
      "avg_word_length    15172 non-null float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "model_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1[model_1['avg_word_length'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "punctuation = punctuation+\"...\"+\"--\"+\"“\"+\"”\"+\"``\"+\"''\"+\"’\"+\"–\"+\"—\"+\"‘\"\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['I’m', 'won’t', '’s', '’ll', '’ve ', 'n’t', '’re', '’d', 'y’all', \"I'm\", \"won't\", \"'s\", \"'ll\", \"'ve \", \"n't\", \"'re\", \"'d\", \"y'all\"])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contr_dict={\"I’m\": \"I am\",\n",
    "            \"won’t\": \"will not\",\n",
    "            \"’s\" : \"\", \n",
    "            \"’ll\":\"will\",\n",
    "            \"’ve \":\"have \",\n",
    "            \"n’t\":\" not\",\n",
    "            \"’re\": \"are\",\n",
    "            \"’d\": \"would\",\n",
    "            \"y’all\": \"all of you\",\n",
    "            \"I'm\": \"I am\",\n",
    "            \"won't\": \"will not\",\n",
    "            \"'s\" : \"\", \n",
    "            \"'ll\":\"will\",\n",
    "            \"'ve \":\"have \",\n",
    "            \"n't\":\"not\",\n",
    "            \"'re\": \"are\",\n",
    "            \"'d\": \"would\",\n",
    "            \"y'all\": \"all of you\"}\n",
    "contr_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(sentence, contr_dict=contr_dict):\n",
    "    for contr in contr_dict.keys():\n",
    "        if contr in sentence:\n",
    "            sentence = sentence.replace(contr,contr_dict[contr])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert nltk tag to wordnet tag\n",
    "# this is important because having the POS tag improves lemmatization\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization and lemmatization function\n",
    "def tokenize_sentence(sentence):\n",
    "    #remove contractions\n",
    "    sentence = replace_contractions(sentence, contr_dict=contr_dict)\n",
    "    \n",
    "    #tokenize the sentence\n",
    "    mytokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    #tag tokens with part of speech\n",
    "    nltk_tagged = nltk.pos_tag(mytokens)\n",
    "    \n",
    "    # remove punctuation\n",
    "    nltk_tagged = [ word for word in nltk_tagged if word[0] not in punctuation ]\n",
    "    \n",
    "    # strip all tokens and make lowercase \n",
    "    nltk_tagged = [ (word[0].lower().strip(),word[1]) for word in nltk_tagged ]\n",
    "    \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_tokens.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(word, tag))\n",
    "            \n",
    "        lemmatized_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-245-d4e5aaf63b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#testing tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_df' is not defined"
     ]
    }
   ],
   "source": [
    "#testing tokenizer\n",
    "tokenize_sentence(text_df['text'].loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y= model_1['propaganda']\n",
    "X = model_1.drop('propaganda',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15172 entries, 0 to 15219\n",
      "Data columns (total 10 columns):\n",
      "text               15172 non-null object\n",
      "sentiment_score    15172 non-null float64\n",
      "abs_sent_score     15172 non-null float64\n",
      "punct_count        15172 non-null int64\n",
      "word_count         15172 non-null int64\n",
      "%adj               15172 non-null float64\n",
      "%verb              15172 non-null float64\n",
      "%adv               15172 non-null float64\n",
      "%noun              15172 non-null float64\n",
      "avg_word_length    15172 non-null float64\n",
      "dtypes: float64(7), int64(2), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Transformations to Features:\n",
    "* TFIDF for text data\n",
    "* Scaling for numerical features like Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\n",
    "#     'This is the first document.',\n",
    "#     'This document is the second document.',\n",
    "#     'And this is the third one.',\n",
    "#     'Is this the first document?',\n",
    "# ]\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(corpus)\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "# print(X.shape)\n",
    "\n",
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = tokenize_sentence, \n",
    "                               min_df=10, max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = X_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_text_train = X_train.drop('text',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_train = scaler.fit_transform(non_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = pd.DataFrame(scaled_train)\n",
    "scaled_train.columns = list(non_text_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.378942</td>\n",
       "      <td>-0.584117</td>\n",
       "      <td>0.107380</td>\n",
       "      <td>1.787735</td>\n",
       "      <td>0.125383</td>\n",
       "      <td>0.155285</td>\n",
       "      <td>-0.298495</td>\n",
       "      <td>0.849950</td>\n",
       "      <td>-0.302474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-0.409388</td>\n",
       "      <td>0.314773</td>\n",
       "      <td>0.438876</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.792049</td>\n",
       "      <td>-0.648888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>0.517110</td>\n",
       "      <td>-0.182099</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.552504</td>\n",
       "      <td>-0.080325</td>\n",
       "      <td>-0.273210</td>\n",
       "      <td>0.703670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>1.336569</td>\n",
       "      <td>0.045189</td>\n",
       "      <td>-0.277755</td>\n",
       "      <td>-0.448369</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.398204</td>\n",
       "      <td>0.460798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-1.091253</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.830264</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.370299</td>\n",
       "      <td>-0.448944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_score  abs_sent_score  punct_count  word_count      %adj  \\\n",
       "0         0.378942       -0.584117     0.107380    1.787735  0.125383   \n",
       "1         0.069278       -1.026921    -0.712079   -0.409388  0.314773   \n",
       "2         0.069278       -1.026921     0.517110   -0.182099 -0.861440   \n",
       "3         0.069278       -1.026921     1.336569    0.045189 -0.277755   \n",
       "4         0.069278       -1.026921    -0.712079   -1.091253 -0.861440   \n",
       "\n",
       "      %verb      %adv     %noun  avg_word_length  \n",
       "0  0.155285 -0.298495  0.849950        -0.302474  \n",
       "1  0.438876 -0.540039  0.792049        -0.648888  \n",
       "2  0.552504 -0.080325 -0.273210         0.703670  \n",
       "3 -0.448369 -0.540039  0.398204         0.460798  \n",
       "4  0.830264 -0.540039  0.370299        -0.448944  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13046    As I wrote in my first analysis piece, the doc...\n",
       "13984    Could you give us a feel for the kind of perso...\n",
       "12566    The New Yorker article, written before Ms. For...\n",
       "9400     [I]f one arrives at the recognition that, in a...\n",
       "14843                          The body has been cremated!\n",
       "4899     When someone is shooting form a horizontal lin...\n",
       "731      This because Trump retweeted three videos show...\n",
       "11010    Bolton says it will be accomplished in the nex...\n",
       "12540    But Campos has been a mysterious subject since...\n",
       "6296     Ostensibly, the next six months are needed to ...\n",
       "10240    Such circular “reasoning” would be amusing if ...\n",
       "1749     Completing this poll grants you access to Free...\n",
       "3862     They can write all big and bad but when it com...\n",
       "9814     do that which is good, and thou shalt have pra...\n",
       "14852    “The shooter’s body was cremated Dec. 21,\" sai...\n",
       "2714     Lew said he feels vindicated to some extent, b...\n",
       "9515     Ever since being soundly beaten in Vietnam and...\n",
       "3396     Daniel Greenfield said this about Dina Habib P...\n",
       "7291     “I hated myself for making the worst decision ...\n",
       "9037                NATO and the EU are showing their age.\n",
       "8541     Completing this poll grants you access to Free...\n",
       "5966     In an interview with Catholic World Report (CW...\n",
       "12060      His goal is to “normalize Jewish anti-Zionism”.\n",
       "3134                                     No, that’s Satan.\n",
       "1159     While there have been countless stories since ...\n",
       "9988     Then he suggests that healing and reconciliati...\n",
       "32       In February 2016, Zakkout circulated on social...\n",
       "14079    Ambellas and Intellihub claim that the FBI and...\n",
       "11812    To keep up everyone’s morale, the museum staff...\n",
       "1340     The woman’s lifeless body can be seen being dr...\n",
       "                               ...                        \n",
       "3393     While President Trump fights to restrict Musli...\n",
       "4564     Article posted with permission from Pamela Geller\n",
       "1188                                                — Sec.\n",
       "6450     We saw high traffic for the search queries ‘vo...\n",
       "5062     Article posted with permission from The Free T...\n",
       "5323     And how was such a depraved individual ordaine...\n",
       "2437                        And I know the pope very well!\n",
       "6980     Many in the hierarchy were active homosexuals ...\n",
       "13850    The Missionary Benedictines in Uznach belong t...\n",
       "10623    Ever thought that the academic discipline of h...\n",
       "770       By that Bryant means “defending British values.”\n",
       "1689     The U.S. government has been putting extraordi...\n",
       "8356     This has been denied by Greg Burke, but given ...\n",
       "11151    The attack took place in a Whataburger restaur...\n",
       "11405    Flake and Corker are being beatified by the Be...\n",
       "11678    James Larkin getting the boot in Dublin for up...\n",
       "14470    Why was the ringleader of the widespread Musli...\n",
       "5590     The government has also agreed to pay a signif...\n",
       "4435     “Recently I have been made aware of what I bel...\n",
       "13573    After Patel's arrest in July, investigators fo...\n",
       "467      Penkoski also shared with Christian Post the p...\n",
       "6295     Notice something else about the deal that Trum...\n",
       "5751     In those theories, the penalty can include san...\n",
       "11326    The year-over- year reduction in the immigrant...\n",
       "12007    Expectations were high in 2014 when the Pope c...\n",
       "5203     In January, Traore was determined to be fit to...\n",
       "13465    Trump wrote of the caravan in a series of twee...\n",
       "5402     In 2,000, after a decade as a public defender,...\n",
       "863      At 17 it seemed such an unlikely thing to thin...\n",
       "7302     Editor’s Note: Over the years, Bishop Robert M...\n",
       "Name: text, Length: 10165, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_train = tfidf_vector.fit_transform(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_text_train_df = pd.DataFrame(tfidf_text_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_text_train_df.columns = list(tfidf_vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10165, 1958)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_text_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>yore</th>\n",
       "      <th>york</th>\n",
       "      <th>youare</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zakkout</th>\n",
       "      <th>zarate</th>\n",
       "      <th>zero</th>\n",
       "      <th>•</th>\n",
       "      <th>…</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1958 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1   10   11   12   13   14   15   16   17   18  ...  yore  york  youare  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0     0.0   \n",
       "\n",
       "   young  youth  zakkout  zarate  zero    •    …  \n",
       "0    0.0    0.0      0.0     0.0   0.0  0.0  0.0  \n",
       "1    0.0    0.0      0.0     0.0   0.0  0.0  0.0  \n",
       "2    0.0    0.0      0.0     0.0   0.0  0.0  0.0  \n",
       "3    0.0    0.0      0.0     0.0   0.0  0.0  0.0  \n",
       "4    0.0    0.0      0.0     0.0   0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 1958 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_text_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_X_trained = tfidf_text_train_df.join(scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10165, 1967)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FT_X_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>…</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.378942</td>\n",
       "      <td>-0.584117</td>\n",
       "      <td>0.107380</td>\n",
       "      <td>1.787735</td>\n",
       "      <td>0.125383</td>\n",
       "      <td>0.155285</td>\n",
       "      <td>-0.298495</td>\n",
       "      <td>0.849950</td>\n",
       "      <td>-0.302474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-0.409388</td>\n",
       "      <td>0.314773</td>\n",
       "      <td>0.438876</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.792049</td>\n",
       "      <td>-0.648888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>0.517110</td>\n",
       "      <td>-0.182099</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.552504</td>\n",
       "      <td>-0.080325</td>\n",
       "      <td>-0.273210</td>\n",
       "      <td>0.703670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>1.336569</td>\n",
       "      <td>0.045189</td>\n",
       "      <td>-0.277755</td>\n",
       "      <td>-0.448369</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.398204</td>\n",
       "      <td>0.460798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069278</td>\n",
       "      <td>-1.026921</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-1.091253</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.830264</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.370299</td>\n",
       "      <td>-0.448944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1967 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1   10   11   12   13   14   15   16   17   18  ...    …  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "   sentiment_score  abs_sent_score  punct_count  word_count      %adj  \\\n",
       "0         0.378942       -0.584117     0.107380    1.787735  0.125383   \n",
       "1         0.069278       -1.026921    -0.712079   -0.409388  0.314773   \n",
       "2         0.069278       -1.026921     0.517110   -0.182099 -0.861440   \n",
       "3         0.069278       -1.026921     1.336569    0.045189 -0.277755   \n",
       "4         0.069278       -1.026921    -0.712079   -1.091253 -0.861440   \n",
       "\n",
       "      %verb      %adv     %noun  avg_word_length  \n",
       "0  0.155285 -0.298495  0.849950        -0.302474  \n",
       "1  0.438876 -0.540039  0.792049        -0.648888  \n",
       "2  0.552504 -0.080325 -0.273210         0.703670  \n",
       "3 -0.448369 -0.540039  0.398204         0.460798  \n",
       "4  0.830264 -0.540039  0.370299        -0.448944  \n",
       "\n",
       "[5 rows x 1967 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FT_X_trained.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = X_test['text']\n",
    "non_text_test = X_test.drop('text',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_test = tfidf_vector.transform(text_test)\n",
    "tfidf_text_test_df = pd.DataFrame(tfidf_text_test.toarray())\n",
    "tfidf_text_test_df.columns = list(tfidf_vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_test = scaler.transform(non_text_test)\n",
    "scaled_test = pd.DataFrame(scaled_test)\n",
    "scaled_test.columns = list(non_text_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_X_test = tfidf_text_test_df.join(scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>…</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.535536</td>\n",
       "      <td>-0.162070</td>\n",
       "      <td>-0.302349</td>\n",
       "      <td>0.499767</td>\n",
       "      <td>0.524811</td>\n",
       "      <td>-0.284494</td>\n",
       "      <td>-0.200727</td>\n",
       "      <td>0.304024</td>\n",
       "      <td>0.439266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785376</td>\n",
       "      <td>-0.002937</td>\n",
       "      <td>0.107380</td>\n",
       "      <td>0.954344</td>\n",
       "      <td>-0.430162</td>\n",
       "      <td>-0.030790</td>\n",
       "      <td>0.726728</td>\n",
       "      <td>-0.371981</td>\n",
       "      <td>-0.120912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.125591</td>\n",
       "      <td>0.681680</td>\n",
       "      <td>1.336569</td>\n",
       "      <td>1.711972</td>\n",
       "      <td>-0.118568</td>\n",
       "      <td>-0.210005</td>\n",
       "      <td>0.278208</td>\n",
       "      <td>-0.766206</td>\n",
       "      <td>-0.598902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.753268</td>\n",
       "      <td>0.149277</td>\n",
       "      <td>-0.302349</td>\n",
       "      <td>-0.030573</td>\n",
       "      <td>-0.155712</td>\n",
       "      <td>-0.793998</td>\n",
       "      <td>-0.021816</td>\n",
       "      <td>0.792049</td>\n",
       "      <td>-0.006964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.514004</td>\n",
       "      <td>-0.192858</td>\n",
       "      <td>-0.712079</td>\n",
       "      <td>-0.863965</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>-1.322372</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.130854</td>\n",
       "      <td>-1.236221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1967 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1   10   11   12   13   14   15   16   17   18  ...    …  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "   sentiment_score  abs_sent_score  punct_count  word_count      %adj  \\\n",
       "0        -0.535536       -0.162070    -0.302349    0.499767  0.524811   \n",
       "1         0.785376       -0.002937     0.107380    0.954344 -0.430162   \n",
       "2        -1.125591        0.681680     1.336569    1.711972 -0.118568   \n",
       "3        -0.753268        0.149277    -0.302349   -0.030573 -0.155712   \n",
       "4        -0.514004       -0.192858    -0.712079   -0.863965 -0.861440   \n",
       "\n",
       "      %verb      %adv     %noun  avg_word_length  \n",
       "0 -0.284494 -0.200727  0.304024         0.439266  \n",
       "1 -0.030790  0.726728 -0.371981        -0.120912  \n",
       "2 -0.210005  0.278208 -0.766206        -0.598902  \n",
       "3 -0.793998 -0.021816  0.792049        -0.006964  \n",
       "4 -1.322372 -0.540039  0.130854        -1.236221  \n",
       "\n",
       "[5 rows x 1967 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FT_X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit/Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf_1 = DummyClassifier(strategy='stratified')\n",
    "dummy_clf_1.fit(FT_X_trained, y_train)\n",
    "\n",
    "dummy_preds_1 = dummy_clf_1.predict(FT_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2510 1000]\n",
      " [1043  454]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.706     0.715     0.711      3510\n",
      "    propaganda      0.312     0.303     0.308      1497\n",
      "\n",
      "      accuracy                          0.592      5007\n",
      "     macro avg      0.509     0.509     0.509      5007\n",
      "  weighted avg      0.589     0.592     0.590      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, dummy_preds_1))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, dummy_preds_1, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf_2 = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf_2.fit(FT_X_trained, y_train)\n",
    "\n",
    "dummy_preds_2 = dummy_clf_2.predict(FT_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3510    0]\n",
      " [1497    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.701     1.000     0.824      3510\n",
      "    propaganda      0.000     0.000     0.000      1497\n",
      "\n",
      "      accuracy                          0.701      5007\n",
      "     macro avg      0.351     0.500     0.412      5007\n",
      "  weighted avg      0.491     0.701     0.578      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, dummy_preds_2))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, dummy_preds_2, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression(C = 3, penalty = 'l2', solver='saga', class_weight = 'balanced')\n",
    "# logreg_cv=GridSearchCV(logreg,grid,cv=10,scoring='f1_weighted')\n",
    "clf_logreg = logreg.fit(FT_X_trained, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_test_preds = clf_logreg.predict(FT_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2350 1160]\n",
      " [ 596  901]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.798     0.670     0.728      3510\n",
      "    propaganda      0.437     0.602     0.506      1497\n",
      "\n",
      "      accuracy                          0.649      5007\n",
      "     macro avg      0.617     0.636     0.617      5007\n",
      "  weighted avg      0.690     0.649     0.662      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, logreg_test_preds))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, logreg_test_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2353 1157]\n",
      " [ 597  900]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.798     0.670     0.728      3510\n",
      "    propaganda      0.438     0.601     0.506      1497\n",
      "\n",
      "      accuracy                          0.650      5007\n",
      "     macro avg      0.618     0.636     0.617      5007\n",
      "  weighted avg      0.690     0.650     0.662      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg_l1=LogisticRegression(C = 3, penalty = 'l1', solver='saga', class_weight = 'balanced')\n",
    "# logreg_cv=GridSearchCV(logreg,grid,cv=10,scoring='f1_weighted')\n",
    "clf_logreg_l1 = logreg.fit(FT_X_trained, y_train)\n",
    "logreg_test_preds_l1 = clf_logreg_l1.predict(FT_X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, logreg_test_preds_l1))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, logreg_test_preds_l1, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_preds = dummy_clf.predict(FT_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=3, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf_svc = svm.SVC(class_weight='balanced',C=3)\n",
    "clf_svc.fit(FT_X_trained, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_preds = clf_svc.predict(FT_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2590  920]\n",
      " [ 606  891]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.810     0.738     0.772      3510\n",
      "    propaganda      0.492     0.595     0.539      1497\n",
      "\n",
      "      accuracy                          0.695      5007\n",
      "     macro avg      0.651     0.667     0.656      5007\n",
      "  weighted avg      0.715     0.695     0.703      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, svc_preds))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, svc_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2826  684]\n",
      " [ 781  716]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.783     0.805     0.794      3510\n",
      "    propaganda      0.511     0.478     0.494      1497\n",
      "\n",
      "      accuracy                          0.707      5007\n",
      "     macro avg      0.647     0.642     0.644      5007\n",
      "  weighted avg      0.702     0.707     0.705      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svc_10 = svm.SVC(class_weight='balanced',C=10)\n",
    "clf_svc_10.fit(FT_X_trained, y_train)\n",
    "svc_preds_10 = clf_svc_10.predict(FT_X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, svc_preds_10))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, svc_preds_10, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2490 1020]\n",
      " [ 579  918]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.811     0.709     0.757      3510\n",
      "    propaganda      0.474     0.613     0.534      1497\n",
      "\n",
      "      accuracy                          0.681      5007\n",
      "     macro avg      0.643     0.661     0.646      5007\n",
      "  weighted avg      0.710     0.681     0.690      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svc_2 = svm.SVC(class_weight='balanced',C=2)\n",
    "clf_svc_2.fit(FT_X_trained, y_train)\n",
    "svc_preds_2 = clf_svc_2.predict(FT_X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, svc_preds_2))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, svc_preds_2, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2261 1249]\n",
      " [ 593  904]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.792     0.644     0.711      3510\n",
      "    propaganda      0.420     0.604     0.495      1497\n",
      "\n",
      "      accuracy                          0.632      5007\n",
      "     macro avg      0.606     0.624     0.603      5007\n",
      "  weighted avg      0.681     0.632     0.646      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svc_01 = svm.SVC(class_weight='balanced',C=.1)\n",
    "clf_svc_01.fit(FT_X_trained, y_train)\n",
    "svc_preds_01 = clf_svc_01.predict(FT_X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, svc_preds_01))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, svc_preds_01, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA output shape: (10165, 200)\n",
      "Sum of explained variance ratio: 93%\n"
     ]
    }
   ],
   "source": [
    "# tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
    "svd = TruncatedSVD(n_components=200, n_iter = 50)\n",
    "# transformed_x_train = tfidf_vec.fit_transform(FT_X_trained)\n",
    "# transformed_x_test = tfidf_vec.transform(FT_X_test)\n",
    "# print('TF-IDF output shape:', transformed_x_train.shape)\n",
    "x_train_svd = svd.fit_transform(FT_X_trained)\n",
    "x_test_svd = svd.transform(FT_X_test)\n",
    "print('LSA output shape:', x_train_svd.shape)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2306 1204]\n",
      " [ 537  960]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.811     0.657     0.726      3510\n",
      "    propaganda      0.444     0.641     0.524      1497\n",
      "\n",
      "      accuracy                          0.652      5007\n",
      "     macro avg      0.627     0.649     0.625      5007\n",
      "  weighted avg      0.701     0.652     0.666      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg_svd=LogisticRegression(C =3, penalty = 'l2', solver='lbfgs', class_weight = 'balanced', max_iter = 500)\n",
    "# logreg_cv=GridSearchCV(logreg,grid,cv=10,scoring='f1_weighted')\n",
    "clf_logreg_svd = logreg_svd.fit(x_train_svd, y_train)\n",
    "logreg_test_preds_svd = clf_logreg_svd.predict(x_test_svd)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, logreg_test_preds_svd))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, logreg_test_preds_svd, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2660     non-propaganda\n",
      "14153    non-propaganda\n",
      "11053    non-propaganda\n",
      "10519    non-propaganda\n",
      "10468    non-propaganda\n",
      "5167     non-propaganda\n",
      "3959     non-propaganda\n",
      "11454    non-propaganda\n",
      "11105        propaganda\n",
      "1089     non-propaganda\n",
      "9904     non-propaganda\n",
      "9494     non-propaganda\n",
      "2485     non-propaganda\n",
      "7855     non-propaganda\n",
      "9029     non-propaganda\n",
      "13076    non-propaganda\n",
      "2452     non-propaganda\n",
      "1354     non-propaganda\n",
      "1724         propaganda\n",
      "6351         propaganda\n",
      "2258     non-propaganda\n",
      "9464         propaganda\n",
      "13937    non-propaganda\n",
      "3536         propaganda\n",
      "8734         propaganda\n",
      "9841         propaganda\n",
      "14121        propaganda\n",
      "10222    non-propaganda\n",
      "3474     non-propaganda\n",
      "9241     non-propaganda\n",
      "11602        propaganda\n",
      "412      non-propaganda\n",
      "4909     non-propaganda\n",
      "13718    non-propaganda\n",
      "11461    non-propaganda\n",
      "9679         propaganda\n",
      "252      non-propaganda\n",
      "14203        propaganda\n",
      "6359     non-propaganda\n",
      "7507         propaganda\n",
      "12474    non-propaganda\n",
      "7989     non-propaganda\n",
      "11797        propaganda\n",
      "1747     non-propaganda\n",
      "2089     non-propaganda\n",
      "1752         propaganda\n",
      "7599         propaganda\n",
      "4472         propaganda\n",
      "12951    non-propaganda\n",
      "6528     non-propaganda\n",
      "Name: propaganda, dtype: object ['non-propaganda' 'non-propaganda' 'non-propaganda' 'propaganda'\n",
      " 'propaganda' 'non-propaganda' 'non-propaganda' 'propaganda' 'propaganda'\n",
      " 'propaganda' 'non-propaganda' 'non-propaganda' 'non-propaganda'\n",
      " 'non-propaganda' 'non-propaganda' 'non-propaganda' 'non-propaganda'\n",
      " 'propaganda' 'non-propaganda' 'non-propaganda' 'non-propaganda'\n",
      " 'propaganda' 'non-propaganda' 'propaganda' 'propaganda' 'non-propaganda'\n",
      " 'non-propaganda' 'propaganda' 'propaganda' 'propaganda' 'non-propaganda'\n",
      " 'non-propaganda' 'propaganda' 'non-propaganda' 'propaganda' 'propaganda'\n",
      " 'non-propaganda' 'non-propaganda' 'non-propaganda' 'propaganda'\n",
      " 'propaganda' 'non-propaganda' 'non-propaganda' 'non-propaganda'\n",
      " 'non-propaganda' 'propaganda' 'propaganda' 'propaganda' 'non-propaganda'\n",
      " 'non-propaganda']\n"
     ]
    }
   ],
   "source": [
    "print(y_test[200:250], logreg_test_preds_svd[200:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Then it’s up to us.'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[11797].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2264 1246]\n",
      " [ 543  954]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.807     0.645     0.717      3510\n",
      "    propaganda      0.434     0.637     0.516      1497\n",
      "\n",
      "      accuracy                          0.643      5007\n",
      "     macro avg      0.620     0.641     0.616      5007\n",
      "  weighted avg      0.695     0.643     0.657      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svd_sgd_clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=500, class_weight = 'balanced')\n",
    "svd_sgd_clf.fit(x_train_svd, y_train)\n",
    "svd_sgd_predicts = svd_sgd_clf.predict(x_test_svd)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, svd_sgd_predicts))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, svd_sgd_predicts, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
