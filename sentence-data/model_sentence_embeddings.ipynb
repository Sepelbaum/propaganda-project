{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROPAGANDA CLASSIFICATION MODEL USING SENTENCE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/91/c85ddef872d5bb39949386930c1f834ac382e145fcd30155b09d6fb65c5a/sentence-transformers-0.2.5.tar.gz (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==2.3.0 (from sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (4.32.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (0.22)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (3.4.4)\n",
      "Collecting sentencepiece (from transformers==2.3.0->sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/56/2e6cfc364c4760b85adab40cb38d91e7ce67d6b2745a2e1aa1497c776fe1/sentencepiece-0.1.85-cp37-cp37m-macosx_10_6_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (1.10.41)\n",
      "Collecting regex!=2019.12.17 (from transformers==2.3.0->sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d9/b58289d885180b5d538aa6df07974b5fe6088547ac846c0f76f77259c304/regex-2020.1.8.tar.gz (681kB)\n",
      "\u001b[K     |████████████████████████████████| 686kB 9.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses (from transformers==2.3.0->sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 9.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.41 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.13.41)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.41->boto3->transformers==2.3.0->sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.41->boto3->transformers==2.3.0->sentence-transformers) (0.14)\n",
      "Building wheels for collected packages: sentence-transformers, regex, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/sashaepelbaum/Library/Caches/pip/wheels/b4/ce/39/5bbda8ac34eb52df8c6531382ca077773fbfcbfb6386e5d66c\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/sashaepelbaum/Library/Caches/pip/wheels/1c/78/87/21be0303007ee5d1483df56703c9c7e5a44873e8f0c51d65f8\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/sashaepelbaum/Library/Caches/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sentence-transformers regex sacremoses\n",
      "Installing collected packages: sentencepiece, regex, sacremoses, transformers, sentence-transformers\n",
      "Successfully installed regex-2020.1.8 sacremoses-0.0.38 sentence-transformers-0.2.5 sentencepiece-0.1.85 transformers-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I optimize the classification model taking in sentence embeddings as features. \n",
    "The order goes as follows:\n",
    "* Transform sentence-text to embeddings using BERT transformer modified by huggingface\n",
    "    * This transformer allows for context-based meaning to enter into the model.\n",
    "    * Since so much of the grammatical structure and tokens seem to overlap between propaganda and non-propaganda sentences, this seems like one of the most promising inputs.\n",
    "* Train-Test split embeddings and corresponding labels\n",
    "* Optimize different classification models. From research it appeared that Logistic Regression and SGD Classifier tend to do better with text-embeddings. In this notebook, I am only including the final model with the best hyper-parameters.\n",
    "\n",
    "\n",
    "Evaluation Metrics:\n",
    "Optimizing for Propaganda-class recall while maintaining a Propaganda-class precicion score above 50. Since Propaganda-class is a minority class (composoing about 30% of the dataset), I wanted to prioritize a model that can identify as many propaganda instances out of the total amount of propaganda instances as possible.\n",
    "\n",
    "The best model ended up being a tuned Logisitc Regression. It reached a Propaganda F1 score of 58 (close to the researcher's value of 60). However, it received substantially higher propaganda recall score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('meta_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Pre-Trained BERT Sentence Embedding Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model as a pickle string. \n",
    "# save the model to disk\n",
    "filename = 'BERT_embeds_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df['text']\n",
    "y_text = df['propaganda']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming text into sentence-embeddings \n",
    "* Usually, we wait to do this after performing train-test-split. However, because the model has been pre-trained, we are merely TRANSFORMING our data. Therefore, we don't need to worry about data-leakage since this occurs when we FIT on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeddings = model.encode(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeds = pd.DataFrame(X_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = [1 if label == 'propaganda' else 0 for label in y_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving embeddings for future use since transformation takes time and computing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_embeddings).to_csv('sentence_embeddings_all.csv')\n",
    "pd.DataFrame(y_labels).to_csv('y_labels_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('sentence_embeddings_all.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15172, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15172, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = pd.read_csv('y_labels_all.csv').drop('Unnamed: 0', axis=1)\n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_train, embeds_test, y_train, y_test = train_test_split(X_embeddings, y_labels, \n",
    "                                                                    test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2509 1001]\n",
      " [ 461 1036]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.845     0.715     0.774      3510\n",
      "           1      0.509     0.692     0.586      1497\n",
      "\n",
      "    accuracy                          0.708      5007\n",
      "   macro avg      0.677     0.703     0.680      5007\n",
      "weighted avg      0.744     0.708     0.718      5007\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7034327915089438"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg2=LogisticRegression(C = 5, penalty = 'l2', solver='newton-cg', class_weight = 'balanced', max_iter = 1000)\n",
    "# logreg_cv=GridSearchCV(logreg,grid,cv=10,scoring='f1_weighted')\n",
    "logreg_embeddings_2 = logreg2.fit(embeds_train, y_train)\n",
    "logreg_embeds_preds_2 = logreg_embeddings_2.predict(embeds_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, logreg_embeds_preds_2))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, logreg_embeds_preds_2, digits=3))\n",
    "\n",
    "roc_auc_score(y_test, logreg_embeds_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2 = 'final_prop_model.sav'\n",
    "pickle.dump(logreg_embeddings_2, open(filename2, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "  \n",
    "# Save the trained model as a pickle string. \n",
    "saved_model_embeds = pickle.dumps(logreg_embeddings_2) \n",
    "  \n",
    "# Load the pickled model \n",
    "log_from_pickle = pickle.loads(saved_model_embeds) \n",
    "  \n",
    "# Use the loaded pickled model to make predictions \n",
    "log_from_pickle.predict(embeds_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
