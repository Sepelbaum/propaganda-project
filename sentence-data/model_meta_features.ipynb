{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROPAGANDA CLASSIFICATION MODEL OF META FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I optimize the classification model taking in engineered meta-features. \n",
    "The order goes as follows:\n",
    "* Train-Test split\n",
    "* Fitting scaler to training data, transforming training and testing data with fit scaler\n",
    "* Optimize different classification models. Optimized a Logistic Regression, Random Forest, Gradient Boosted Decision Tree. I include the optimized model for all three alogrithms. \n",
    "\n",
    "\n",
    "Evaluation Metrics:\n",
    "Optimizing for Propaganda-class recall while maintaining a Propaganda-class precicion score above 50. Since Propaganda-class is a minority class (composoing about 30% of the dataset), I wanted to prioritize a model that can identify as many propaganda instances out of the total amount of propaganda instances as possible.\n",
    "\n",
    "The best model ended up being a tuned Random Forest. However, since I created a stacked model that needed to output probabilities, I will use the best version of the Logistic Regression for the stacked model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Meta-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('meta_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>propaganda</th>\n",
       "      <th>propaganda_type</th>\n",
       "      <th>text</th>\n",
       "      <th>prop_txt_snippet</th>\n",
       "      <th>sent_#</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>strong_subjectives_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Florida Muslim Leader Sofian Zakkout’s D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.444444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>701225819</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Name_Calling,Labeling</td>\n",
       "      <td>David Duke, the white supremacist icon and for...</td>\n",
       "      <td>Grand Wizard of the Ku Klux Klan</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>4.423077</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>701225819</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>However, one individual who represents the Mus...</td>\n",
       "      <td>enamored</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Last month, once again, Zakkout chose to showc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701225819</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The postings can be rivaled only by Zakkout’s ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id      propaganda        propaganda_type  \\\n",
       "0   701225819  non-propaganda                    NaN   \n",
       "1   701225819      propaganda  Name_Calling,Labeling   \n",
       "2   701225819      propaganda        Loaded_Language   \n",
       "3   701225819  non-propaganda                    NaN   \n",
       "4   701225819  non-propaganda                    NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  South Florida Muslim Leader Sofian Zakkout’s D...   \n",
       "1  David Duke, the white supremacist icon and for...   \n",
       "2  However, one individual who represents the Mus...   \n",
       "3  Last month, once again, Zakkout chose to showc...   \n",
       "4  The postings can be rivaled only by Zakkout’s ...   \n",
       "\n",
       "                   prop_txt_snippet  sent_#  sentiment_score  abs_sent_score  \\\n",
       "0                               NaN       1           0.0000          0.0000   \n",
       "1  Grand Wizard of the Ku Klux Klan       2           0.5423          0.5423   \n",
       "2                          enamored       3           0.3612          0.3612   \n",
       "3                               NaN       4           0.0000          0.0000   \n",
       "4                               NaN       5           0.0000          0.0000   \n",
       "\n",
       "   punct_count  word_count      %adj     %verb      %adv     %noun  \\\n",
       "0            0           9  0.000000  0.000000  0.000000  0.000000   \n",
       "1            4          26  0.020548  0.006849  0.013699  0.006849   \n",
       "2            4          27  0.017241  0.017241  0.005747  0.022989   \n",
       "3            5          22  0.021127  0.021127  0.014085  0.035211   \n",
       "4            1          11  0.014493  0.043478  0.014493  0.028986   \n",
       "\n",
       "   avg_word_length  strong_subjectives_count  \n",
       "0         5.444444                         0  \n",
       "1         4.423077                         2  \n",
       "2         5.000000                         0  \n",
       "3         5.045455                         0  \n",
       "4         4.636364                         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Non-Meta and Deterministic Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = df.drop(['propaganda_type','text','prop_txt_snippet','sent_#','article_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previewing Final DataFrame and Missing Values Before Diving In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>propaganda</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>abs_sent_score</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>%adj</th>\n",
       "      <th>%verb</th>\n",
       "      <th>%adv</th>\n",
       "      <th>%noun</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>strong_subjectives_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.444444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>propaganda</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>4.423077</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>propaganda</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       propaganda  sentiment_score  abs_sent_score  punct_count  word_count  \\\n",
       "0  non-propaganda           0.0000          0.0000            0           9   \n",
       "1      propaganda           0.5423          0.5423            4          26   \n",
       "2      propaganda           0.3612          0.3612            4          27   \n",
       "3  non-propaganda           0.0000          0.0000            5          22   \n",
       "4  non-propaganda           0.0000          0.0000            1          11   \n",
       "\n",
       "       %adj     %verb      %adv     %noun  avg_word_length  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000         5.444444   \n",
       "1  0.020548  0.006849  0.013699  0.006849         4.423077   \n",
       "2  0.017241  0.017241  0.005747  0.022989         5.000000   \n",
       "3  0.021127  0.021127  0.014085  0.035211         5.045455   \n",
       "4  0.014493  0.043478  0.014493  0.028986         4.636364   \n",
       "\n",
       "   strong_subjectives_count  \n",
       "0                         0  \n",
       "1                         2  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15172 entries, 0 to 15171\n",
      "Data columns (total 11 columns):\n",
      "propaganda                  15172 non-null object\n",
      "sentiment_score             15172 non-null float64\n",
      "abs_sent_score              15172 non-null float64\n",
      "punct_count                 15172 non-null int64\n",
      "word_count                  15172 non-null int64\n",
      "%adj                        15172 non-null float64\n",
      "%verb                       15172 non-null float64\n",
      "%adv                        15172 non-null float64\n",
      "%noun                       15172 non-null float64\n",
      "avg_word_length             15172 non-null float64\n",
      "strong_subjectives_count    15172 non-null int64\n",
      "dtypes: float64(7), int64(3), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = meta_df['propaganda']\n",
    "X = meta_df.drop('propaganda', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1 if label == 'propaganda' else 0 for label in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "dummy_preds = dummy_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3510    0]\n",
      " [1497    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.701     1.000     0.824      3510\n",
      "           1      0.000     0.000     0.000      1497\n",
      "\n",
      "    accuracy                          0.701      5007\n",
      "   macro avg      0.351     0.500     0.412      5007\n",
      "weighted avg      0.491     0.701     0.578      5007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, dummy_preds))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, dummy_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c6cd85f0f09f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "roc_auc_score(y_test, dummy_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_grid_logistic = {'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'class_weight': 'balanced'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create regularization penalty space\n",
    "# penalty = ['l1', 'l2']\n",
    "\n",
    "# # Create regularization hyperparameter distribution using uniform distribution\n",
    "# C = uniform(loc=0, scale=4)\n",
    "\n",
    "# # Create hyperparameter options\n",
    "# hyperparameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf_log = RandomizedSearchCV(logistic, hyperparam_grid_logistic, random_state=1, n_iter=200, cv=5, \n",
    "                         verbose=True, n_jobs=-1, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   41.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   53.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Fit randomized search\n",
    "best_model_log = clf_log.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 0.012742749857031334\n",
      "Best solver: sag\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model_log.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model_log.best_estimator_.get_params()['C'])\n",
    "print('Best solver:', best_model_log.best_estimator_.get_params()['solver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target vector\n",
    "log_preds = best_model_log.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3347  163]\n",
      " [1267  230]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.725     0.954     0.824      3510\n",
      "           1      0.585     0.154     0.243      1497\n",
      "\n",
      "    accuracy                          0.714      5007\n",
      "   macro avg      0.655     0.554     0.534      5007\n",
      "weighted avg      0.683     0.714     0.650      5007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, log_preds))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, log_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553600934061856"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, log_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_grid_rf=    {'n_estimators' : list(range(10,101,10)),\n",
    "    'max_features' : list(range(6,32,5)),\n",
    "    'criterion':['gini','entropy'],\n",
    "    'class_weight':['balanced']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomizedSearchCV(randomforest, hyperparam_grid_rf, random_state=1, n_iter=200, cv=5, \n",
    "                         verbose=True, n_jobs=-1, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 120 is smaller than n_iter=200. Running 120 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit randomized search\n",
    "best_model_rf = clf_rf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: 30\n",
      "Best C: 6\n",
      "Best solver: entropy\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model_rf.best_estimator_.get_params()['n_estimators'])\n",
    "print('Best C:', best_model_rf.best_estimator_.get_params()['max_features'])\n",
    "print('Best solver:', best_model_rf.best_estimator_.get_params()['criterion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3196  314]\n",
      " [1153  344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.735     0.911     0.813      3510\n",
      "           1      0.523     0.230     0.319      1497\n",
      "\n",
      "    accuracy                          0.707      5007\n",
      "   macro avg      0.629     0.570     0.566      5007\n",
      "weighted avg      0.671     0.707     0.666      5007\n",
      "\n",
      "0.5701671148564936\n"
     ]
    }
   ],
   "source": [
    "# Predict target vector\n",
    "rf_preds = best_model_rf.predict(X_test_scaled)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, rf_preds, digits=3))\n",
    "\n",
    "print(roc_auc_score(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gboost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_model = clf_gboost.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3295  215]\n",
      " [1176  321]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.737     0.939     0.826      3510\n",
      "           1      0.599     0.214     0.316      1497\n",
      "\n",
      "    accuracy                          0.722      5007\n",
      "   macro avg      0.668     0.577     0.571      5007\n",
      "weighted avg      0.696     0.722     0.673      5007\n",
      "\n",
      "0.5765876482309348\n"
     ]
    }
   ],
   "source": [
    "# Predict target vector\n",
    "gboost_preds = gboost_model.predict(X_test_scaled)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, gboost_preds))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, gboost_preds, digits=3))\n",
    "\n",
    "print(roc_auc_score(y_test, gboost_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
