{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/91/c85ddef872d5bb39949386930c1f834ac382e145fcd30155b09d6fb65c5a/sentence-transformers-0.2.5.tar.gz (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==2.3.0 (from sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (4.32.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (0.22)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (3.4.4)\n",
      "Collecting sentencepiece (from transformers==2.3.0->sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/56/2e6cfc364c4760b85adab40cb38d91e7ce67d6b2745a2e1aa1497c776fe1/sentencepiece-0.1.85-cp37-cp37m-macosx_10_6_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (1.10.41)\n",
      "Collecting regex!=2019.12.17 (from transformers==2.3.0->sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d9/b58289d885180b5d538aa6df07974b5fe6088547ac846c0f76f77259c304/regex-2020.1.8.tar.gz (681kB)\n",
      "\u001b[K     |████████████████████████████████| 686kB 9.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses (from transformers==2.3.0->sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 9.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.41 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.13.41)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.41->boto3->transformers==2.3.0->sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /Users/sashaepelbaum/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.41->boto3->transformers==2.3.0->sentence-transformers) (0.14)\n",
      "Building wheels for collected packages: sentence-transformers, regex, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/sashaepelbaum/Library/Caches/pip/wheels/b4/ce/39/5bbda8ac34eb52df8c6531382ca077773fbfcbfb6386e5d66c\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/sashaepelbaum/Library/Caches/pip/wheels/1c/78/87/21be0303007ee5d1483df56703c9c7e5a44873e8f0c51d65f8\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/sashaepelbaum/Library/Caches/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sentence-transformers regex sacremoses\n",
      "Installing collected packages: sentencepiece, regex, sacremoses, transformers, sentence-transformers\n",
      "Successfully installed regex-2020.1.8 sacremoses-0.0.38 sentence-transformers-0.2.5 sentencepiece-0.1.85 transformers-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentence_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405M/405M [01:07<00:00, 6.01MB/s] \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df['text']\n",
    "y_text = df['propaganda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, y_train, y_test = train_test_split(X_text, y_text, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10197,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = np.array(sentences_train)\n",
    "sentences_test = np.array(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_train = model.encode(sentences_train)\n",
    "sentence_embeddings_test = model.encode(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_embeddings_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2609  959]\n",
      " [ 454 1001]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.852     0.731     0.787      3568\n",
      "    propaganda      0.511     0.688     0.586      1455\n",
      "\n",
      "      accuracy                          0.719      5023\n",
      "     macro avg      0.681     0.710     0.687      5023\n",
      "  weighted avg      0.753     0.719     0.729      5023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression(C = 3, penalty = 'l2', solver='newton-cg', class_weight = 'balanced', max_iter = 1000)\n",
    "# logreg_cv=GridSearchCV(logreg,grid,cv=10,scoring='f1_weighted')\n",
    "logreg_embeddings_1 = logreg.fit(sentence_embeddings_train, y_train)\n",
    "logreg_embeddings_preds_1 = logreg_embeddings_1.predict(sentence_embeddings_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, logreg_embeddings_preds_1))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, logreg_embeddings_preds_1, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2612  956]\n",
      " [ 455 1000]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.852     0.732     0.787      3568\n",
      "    propaganda      0.511     0.687     0.586      1455\n",
      "\n",
      "      accuracy                          0.719      5023\n",
      "     macro avg      0.681     0.710     0.687      5023\n",
      "  weighted avg      0.753     0.719     0.729      5023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg2=LogisticRegression(C = 5, penalty = 'l2', solver='newton-cg', class_weight = 'balanced', max_iter = 1000)\n",
    "# logreg_cv=GridSearchCV(logreg,grid,cv=10,scoring='f1_weighted')\n",
    "logreg_embeddings_2 = logreg2.fit(sentence_embeddings_train, y_train)\n",
    "logreg_embeddings_preds_2 = logreg_embeddings_2.predict(sentence_embeddings_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, logreg_embeddings_preds_2))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, logreg_embeddings_preds_2, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2101 1467]\n",
      " [ 318 1137]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda      0.869     0.589     0.702      3568\n",
      "    propaganda      0.437     0.781     0.560      1455\n",
      "\n",
      "      accuracy                          0.645      5023\n",
      "     macro avg      0.653     0.685     0.631      5023\n",
      "  weighted avg      0.743     0.645     0.661      5023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svd_sgd_clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=200, class_weight = 'balanced')\n",
    "svd_sgd_clf.fit(sentence_embeddings_train, y_train)\n",
    "svd_sgd_predicts_embeddings = svd_sgd_clf.predict(sentence_embeddings_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(sklearn.metrics.confusion_matrix(y_test, svd_sgd_predicts_embeddings))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(sklearn.metrics.classification_report(y_test, svd_sgd_predicts_embeddings, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
